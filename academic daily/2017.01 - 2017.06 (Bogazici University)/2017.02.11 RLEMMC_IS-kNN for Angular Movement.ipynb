{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rlemmc import environment, policy, trajectory, montecarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment\n",
    "horizon = 10\n",
    "env = environment.AngularMovement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "sampleCount = 100\n",
    "iterationCount = 100\n",
    "selectedInference = montecarlo.importance_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Policy\n",
    "kNearest = 5\n",
    "p = policy.KnnPolicyContinuous(kNearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(states, color='red'):\n",
    "\n",
    "    [sample_count, _, _] = states.shape\n",
    "\n",
    "    for s in range(sample_count):\n",
    "        plt.plot(states[s, :, 0], states[s, :, 1], '-', color=color)\n",
    "        plt.plot(states[s, :, 0], states[s, :, 1], 'o', color=color)\n",
    "        plt.plot(states[s, -1, 0], states[s, -1, 1], 'o', color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_iteration(states, newStates):\n",
    "    plot_trajectories(states, color='red')\n",
    "    plot_trajectories(newStates, color='green')\n",
    "    plt.plot(env.targetState[0], env.targetState[1], 'o', color='blue', markersize=20)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Average Reward : 0.0018033297931797552\n",
      "Iteration 2 - Average Reward : 0.10143965046892006\n",
      "Iteration 3 - Average Reward : 0.19111506545831614\n",
      "Iteration 4 - Average Reward : 0.1766406643583903\n",
      "Iteration 5 - Average Reward : 0.1745899855592202\n",
      "Iteration 6 - Average Reward : 0.22885611304261647\n",
      "Iteration 7 - Average Reward : 0.24223060598027615\n",
      "Iteration 8 - Average Reward : 0.24005548183781045\n",
      "Iteration 9 - Average Reward : 0.26953042824462914\n",
      "Iteration 10 - Average Reward : 0.26973754238291203\n",
      "Iteration 11 - Average Reward : 0.30664679400148276\n",
      "Iteration 12 - Average Reward : 0.28596407239288524\n",
      "Iteration 13 - Average Reward : 0.3334368822073654\n",
      "Iteration 14 - Average Reward : 0.4050659731439192\n",
      "Iteration 15 - Average Reward : 0.50148404925754\n",
      "Iteration 16 - Average Reward : 0.49074668166701685\n",
      "Iteration 17 - Average Reward : 0.5361835724599693\n",
      "Iteration 18 - Average Reward : 0.5707763702518044\n",
      "Iteration 19 - Average Reward : 0.6169011852160771\n",
      "Iteration 20 - Average Reward : 0.6278488862518904\n",
      "Iteration 21 - Average Reward : 0.6348756094697968\n",
      "Iteration 22 - Average Reward : 0.6652035702240291\n",
      "Iteration 23 - Average Reward : 0.6608048288564257\n",
      "Iteration 24 - Average Reward : 0.7031704791727297\n",
      "Iteration 25 - Average Reward : 0.6902968347366262\n",
      "Iteration 26 - Average Reward : 0.7274372067690613\n",
      "Iteration 27 - Average Reward : 0.733844769025686\n",
      "Iteration 28 - Average Reward : 0.7257562404714197\n",
      "Iteration 29 - Average Reward : 0.7313975374873722\n",
      "Iteration 30 - Average Reward : 0.7233016095883368\n",
      "Iteration 31 - Average Reward : 0.7716102195806276\n",
      "Iteration 32 - Average Reward : 0.7537363327775667\n",
      "Iteration 33 - Average Reward : 0.7530168520404352\n",
      "Iteration 34 - Average Reward : 0.7433600208759763\n",
      "Iteration 35 - Average Reward : 0.7103029440270029\n",
      "Iteration 36 - Average Reward : 0.7433810128067802\n",
      "Iteration 37 - Average Reward : 0.7787271221700505\n",
      "Iteration 38 - Average Reward : 0.7620040339310511\n",
      "Iteration 39 - Average Reward : 0.7497384556728338\n",
      "Iteration 40 - Average Reward : 0.7521173606264542\n",
      "Iteration 41 - Average Reward : 0.7683313285707482\n",
      "Iteration 42 - Average Reward : 0.7420209008891939\n",
      "Iteration 43 - Average Reward : 0.7949483981881157\n",
      "Iteration 44 - Average Reward : 0.7372151134908749\n",
      "Iteration 45 - Average Reward : 0.7766101301099197\n",
      "Iteration 46 - Average Reward : 0.7744839636132723\n",
      "Iteration 47 - Average Reward : 0.718634448375995\n",
      "Iteration 48 - Average Reward : 0.7794913965690465\n",
      "Iteration 49 - Average Reward : 0.767872512458541\n",
      "Iteration 50 - Average Reward : 0.7357100497761764\n",
      "Iteration 51 - Average Reward : 0.7449286735140404\n",
      "Iteration 52 - Average Reward : 0.796363607042331\n",
      "Iteration 53 - Average Reward : 0.7482329379059303\n",
      "Iteration 54 - Average Reward : 0.7656839072000742\n",
      "Iteration 55 - Average Reward : 0.7222303237752928\n",
      "Iteration 56 - Average Reward : 0.7642515418679712\n",
      "Iteration 57 - Average Reward : 0.7404488074854892\n",
      "Iteration 58 - Average Reward : 0.7707469756281659\n",
      "Iteration 59 - Average Reward : 0.7724283898413091\n",
      "Iteration 60 - Average Reward : 0.7765293093833552\n",
      "Iteration 61 - Average Reward : 0.6843189095183169\n",
      "Iteration 62 - Average Reward : 0.748050663808099\n"
     ]
    }
   ],
   "source": [
    "iterationRewards = []\n",
    "\n",
    "for i in range(iterationCount):\n",
    "\n",
    "    if i == 0:\n",
    "        iterationPolicy = policy.UniformPolicyContinuous()\n",
    "    else:\n",
    "        iterationPolicy = p\n",
    "\n",
    "    [states, actions, rewards] = \\\n",
    "        trajectory.rollout_trajectories(env, iterationPolicy, horizon, sampleCount)\n",
    "\n",
    "    # E-Step\n",
    "    [newStates, newActions] = selectedInference(states, actions, rewards[:,-1])\n",
    "\n",
    "    # M-Step\n",
    "    p.m_step(newStates, newActions)\n",
    "    \n",
    "    # Plot Trajectories\n",
    "    # plot_iteration(states, newStates)\n",
    "    \n",
    "    # Average Reward\n",
    "    iterationRewards.append(np.mean(rewards[:,-1]))\n",
    "    print( f'Iteration {i+1} - Average Reward : {iterationRewards[i]}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(iterationRewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_iteration(states,newStates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
