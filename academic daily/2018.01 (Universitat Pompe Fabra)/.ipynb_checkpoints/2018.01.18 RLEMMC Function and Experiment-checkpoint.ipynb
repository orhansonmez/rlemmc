{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Cart Pole\n",
    "\n",
    "This is the noisy version of the CartPole-v0 environment of OpenAI.  \n",
    "https://gym.openai.com/envs/CartPole-v0  \n",
    "https://github.com/openai/gym/wiki/CartPole-v0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoisyCartPoleEnvironment:\n",
    "    \n",
    "    state_dimension = 4\n",
    "    action_dimension = 1\n",
    "    action_space = range(2)\n",
    "    transition_sigmas = [ 1e-2, 1e-5, 1e-2, 1e-5 ]\n",
    "    transition_covariance = np.diagflat(transition_sigmas)\n",
    "    logp_mean = (-1/2) * np.log(np.linalg.det(2*np.pi*transition_covariance))\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cartpole_reset(self):\n",
    "        state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        return np.array(state)\n",
    "    \n",
    "    # Extracted from OpenAI environment CartPole-v0\n",
    "    def cartpole_step(self, state, action):\n",
    "\n",
    "        gravity = 9.8\n",
    "        masscart = 1.0\n",
    "        masspole = 0.1\n",
    "        total_mass = (masspole + masscart)\n",
    "        length = 0.5 # actually half the pole's length\n",
    "        polemass_length = (masspole * length)\n",
    "        force_mag = 10.0\n",
    "        tau = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        x_threshold = 2.4\n",
    "\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "\n",
    "        already_done =  x < -x_threshold \\\n",
    "            or x > x_threshold \\\n",
    "            or theta < -theta_threshold_radians \\\n",
    "            or theta > theta_threshold_radians\n",
    "        already_done = bool(already_done)\n",
    "\n",
    "        if already_done:\n",
    "\n",
    "            next_state = state\n",
    "            reward = 0\n",
    "            done = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            force = force_mag if action==1 else -force_mag\n",
    "            costheta = math.cos(theta)\n",
    "            sintheta = math.sin(theta)\n",
    "            temp = (force + polemass_length * theta_dot * theta_dot * sintheta) / total_mass\n",
    "            thetaacc = (gravity * sintheta - costheta* temp) / (length * (4.0/3.0 - masspole * costheta * costheta / total_mass))\n",
    "            xacc  = temp - polemass_length * thetaacc * costheta / total_mass\n",
    "            x  = x + tau * x_dot\n",
    "            x_dot = x_dot + tau * xacc\n",
    "            theta = theta + tau * theta_dot\n",
    "            theta_dot = theta_dot + tau * thetaacc\n",
    "            next_state = np.array([x,x_dot,theta,theta_dot])\n",
    "\n",
    "            reward = 1\n",
    "\n",
    "            done =  x < -x_threshold \\\n",
    "                or x > x_threshold \\\n",
    "                or theta < -theta_threshold_radians \\\n",
    "                or theta > theta_threshold_radians\n",
    "            done = bool(done)\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def noisycartpole_reset(self):\n",
    "        return self.cartpole_reset()\n",
    "\n",
    "    def noisycartpole_step(self, state, action):\n",
    "\n",
    "        next_state_mean, reward, done, info = env.cartpole_step(state, action)   # CartPole Step\n",
    "\n",
    "        if not done:\n",
    "            noise = np.random.randn(env.state_dimension) * env.transition_sigmas        # Adding Noise\n",
    "            logp = multivariate_normal.logpdf( next_state_mean + noise, mean=next_state_mean, cov=env.transition_covariance)\n",
    "        else:\n",
    "            noise = np.zeros(env.state_dimension)\n",
    "            logp = env.logp_mean\n",
    "        \n",
    "        return next_state_mean + noise, reward, done, logp\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.noisycartpole_reset()\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        return self.noisycartpole_step(state, action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = NoisyCartPoleEnvironment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trajectory2tuples(states, actions):\n",
    "\n",
    "    # Dimensions\n",
    "    [sample_count, horizon, state_dimension] = states.shape\n",
    "    [_, _, action_dimension] = actions.shape\n",
    "\n",
    "    # Reshape Inputs and Targets\n",
    "    inputs = np.reshape(states, (sample_count*horizon, state_dimension))\n",
    "    targets = np.reshape(actions, (sample_count*horizon, action_dimension))\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rollout_trajectories(env, policy, horizon, sample_count=1, init=None):\n",
    "\n",
    "    # States and Actions\n",
    "    states = np.zeros((sample_count, horizon, env.state_dimension))\n",
    "    actions = np.zeros((sample_count, horizon, env.action_dimension))\n",
    "    rewards = np.zeros((sample_count, horizon))\n",
    "    logp = np.zeros(sample_count)\n",
    "    \n",
    "    # Sample Trajectories\n",
    "    for t in range(horizon):\n",
    "        \n",
    "        logp_step_transition = np.zeros((sample_count))\n",
    "\n",
    "        # Initialization\n",
    "        if t == 0:\n",
    "            if init is None:\n",
    "                states[:,t,:] = [ env.reset() for i in range(sample_count) ]\n",
    "            else:\n",
    "                states[:,t,:] = init\n",
    "                \n",
    "        # Transition and Reward\n",
    "        else:\n",
    "            for s in range(sample_count):\n",
    "                states[s, t, :], rewards[s,t-1], _1, logp_step_transition[s] = env.step(states[s, t-1, :], actions[s, t-1, :])\n",
    "        \n",
    "        # Action Selection\n",
    "        actions_unshaped, logp_step_policy = policy.query(states[:, t, :])\n",
    "        actions[:,t,:] = actions_unshaped.reshape(sample_count, env.action_dimension)\n",
    "        \n",
    "        # Log Probability of Sampling\n",
    "        logp += logp_step_transition + logp_step_policy\n",
    "        \n",
    "    for s in range(sample_count):\n",
    "        _, rewards[s, horizon-1], _1, _2 = env.step(states[s, horizon-1, :], actions[s, horizon-1, :])\n",
    "    \n",
    "    return states, actions, rewards, logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SciKitPolicy():\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def query(self, states):\n",
    "        if len(states.shape) == 1:\n",
    "            states = states.reshape(1, -1)\n",
    "        return self.method.predict(states), np.zeros(states.shape[0])\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        self.method.fit(inputs, targets)\n",
    "\n",
    "    def m_step(self, states, actions):\n",
    "\n",
    "        # States/Actions -> Inputs/Targets\n",
    "        inputs, targets = trajectory2tuples(states, actions)\n",
    "\n",
    "        # Train kNN\n",
    "        self.train(inputs, targets.ravel())\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KnnPolicyDiscrete(SciKitPolicy):\n",
    "    def __init__(self, k, weights='distance'):\n",
    "        self.method = KNeighborsClassifier(n_neighbors=k, weights=weights, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UniformPolicyDiscrete():\n",
    "\n",
    "    def __init__(self, choices):\n",
    "        self.choices = choices\n",
    "\n",
    "    def query(self, states):\n",
    "        return np.random.choice(self.choices, size=states.shape[0]), np.zeros(states.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLEMMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment  \n",
    "horizon = 25\n",
    "\n",
    "# Inference\n",
    "iteration_count = 10\n",
    "sample_count = 100\n",
    "burn_in = 10\n",
    "\n",
    "# Policy\n",
    "k_nearest = 5\n",
    "policy_sample_count = sample_count\n",
    "policy_approximation = KnnPolicyDiscrete(k_nearest)\n",
    "\n",
    "# Bridge Functions\n",
    "temperatures = [0, 0.5, 1 ]\n",
    "temperature_count = len(temperatures)\n",
    "eta = lambda n : temperatures[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal Functions for MCMC and SIMCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proposal_prior(states_previous, actions_previous, rewards_previous, logp_previous, policy):\n",
    "    return rollout_trajectories(env, policy, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proposal_prior_cached(_1, _2, _3, _4, policy):\n",
    "    global states_cached, actions_cached, rewards_cached, logp_cached, cache_index\n",
    "    \n",
    "    if cache_index < 0:        \n",
    "        states_cached, actions_cached, rewards_cached, logp_cached = \\\n",
    "            rollout_trajectories(env, policy, horizon, sample_count=sample_count * temperature_count + burn_in)\n",
    "    \n",
    "    cache_index +=1\n",
    "    \n",
    "    return states_cached[cache_index], actions_cached[cache_index], rewards_cached[cache_index], logp_cached[cache_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proposal_keepthenprior(states, actions, rewards, policy):\n",
    "    \n",
    "    t = np.random.randint(states.shape[0]+1) -1\n",
    "    \n",
    "    if t < 0:\n",
    "        states, actions, rewards = \\\n",
    "            rollout_trajectories(env, policy, horizon )\n",
    "    else:\n",
    "        states[t:,:], actions[t:,:], rewards[t:] = \\\n",
    "            rollout_trajectories(env, policy, horizon-t, init=states[t])\n",
    "    \n",
    "    return states, actions, rewards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Kernels for SMCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_noop(env, policy, states_prev, actions_prev, rewards_prev, logp_prev):\n",
    "    return states_prev, actions_prev, rewards_prev, logp_prev, [int(x) for x in range(sample_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_prior(env, policy, states_prev, actions_prev, rewards_prev, logp_prev):\n",
    "    \n",
    "    ancestors = [np.random.randint(sample_count) for x in range(sample_count)]\n",
    "    states, actions, rewards, logp = rollout_trajectories(env, policy, horizon, sample_count)\n",
    "    \n",
    "    return states, actions, rewards, logp, ancestors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def importance_sampling(env, policy, parameters):\n",
    "    list(map(exec,[\"{0}={1}\".format(k,v) for k,v in parameters.items()]))\n",
    "        \n",
    "    states, actions, rewards, _ = \\\n",
    "            rollout_trajectories(env, policy, horizon, sample_count)\n",
    "    \n",
    "    # Weighting\n",
    "    if np.sum(rewards) == 0:\n",
    "        weights = np.ones(sample_count) / sample_count\n",
    "    else:\n",
    "        weights = np.sum(rewards,axis=1) / np.sum(rewards)\n",
    "\n",
    "    # Resampling\n",
    "    index = np.random.choice(range(sample_count), size=policy_sample_count, p=weights, replace=True)\n",
    "\n",
    "    # New Trajectories\n",
    "    states_new = np.zeros((policy_sample_count, horizon, env.state_dimension))\n",
    "    actions_new = np.zeros((policy_sample_count, horizon, env.action_dimension))\n",
    "    rewards_new = np.zeros((policy_sample_count, horizon))\n",
    "    \n",
    "    for s in range(policy_sample_count):\n",
    "        states_new[s] = states[index[s], :, :]\n",
    "        actions_new[s] = actions[index[s], :, :]\n",
    "        rewards_new[s] = rewards[index[s], : ]\n",
    "\n",
    "    return [states_new, actions_new, rewards_new ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mcmc(env, policy, parameters):\n",
    "    list(map(exec,[\"{0}={1}\".format(k,v) for k,v in parameters.items()]))\n",
    "    \n",
    "    # States and Actions\n",
    "    states = np.zeros((temperature_count * sample_count, horizon, env.state_dimension))\n",
    "    actions = np.zeros((temperature_count * sample_count, horizon, env.action_dimension))\n",
    "    rewards = np.zeros((temperature_count *sample_count, horizon))\n",
    "\n",
    "    for i in range(temperature_count * sample_count):\n",
    "        \n",
    "        if i == 0:\n",
    "            states[i,:,:], actions[i,:,:], rewards[i,:], _ = \\\n",
    "                rollout_trajectories(env, policy, horizon)\n",
    "        else:\n",
    "            states_candidate, actions_candidate, rewards_candidate, _ = \\\n",
    "                proposal_prior_cached(states[i-1,:,:], actions[i-1,:,:], rewards[i-1,:], [], policy)\n",
    "            \n",
    "            reward_previous = np.sum(rewards[i-1,:])    \n",
    "            reward_candidate = np.sum(rewards_candidate)\n",
    "\n",
    "            alpha = reward_candidate / reward_previous\n",
    "\n",
    "            if np.random.rand() < alpha:\n",
    "                states[i,:,:] = states_candidate\n",
    "                actions[i,:,:] = actions_candidate\n",
    "                rewards[i,:] = rewards_candidate\n",
    "            else:\n",
    "                states[i,:,:] = states[i-1,:,:]\n",
    "                actions[i,:,:] = actions[i-1,:,:]\n",
    "                rewards[i,:] = rewards[i-1,:]\n",
    "        \n",
    "         # Sampling\n",
    "        index = np.random.choice(range(temperature_count * sample_count - burn_in), size=policy_sample_count, replace=False)\n",
    "\n",
    "        # Policy Trajectories\n",
    "        states_new = np.zeros((policy_sample_count, horizon, env.state_dimension))\n",
    "        actions_new = np.zeros((policy_sample_count, horizon, env.action_dimension))\n",
    "        rewards_new = np.zeros((policy_sample_count, horizon))\n",
    "        for s in range(policy_sample_count):\n",
    "            states_new[s] = states[index[s] + burn_in, :, :]\n",
    "            actions_new[s] = actions[index[s] + burn_in, :, :]\n",
    "            rewards_new[s] = rewards[index[s] + burn_in, : ]\n",
    "        \n",
    "    return states_new, actions_new, rewards_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Monte Carlo Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smcs(env, policy, parameters):\n",
    "    list(map(exec,[\"{0}={1}\".format(k,v) for k,v in parameters.items()]))\n",
    "    \n",
    "    print(test_variable)\n",
    "    \n",
    "    states = np.zeros((temperature_count, sample_count, horizon, env.state_dimension))\n",
    "    actions = np.zeros((temperature_count, sample_count, horizon, env.action_dimension))\n",
    "    rewards = np.zeros((temperature_count, sample_count, horizon))\n",
    "    logp = np.zeros((temperature_count, sample_count))\n",
    "    ancestors = np.zeros((temperature_count, sample_count))\n",
    "\n",
    "    for n in range(temperature_count):\n",
    "    \n",
    "        if n == 0:\n",
    "            # Initial Trajectories\n",
    "            states[n], actions[n], rewards[n], logp[n] = \\\n",
    "                rollout_trajectories(env, policy, horizon, sample_count)\n",
    "        else:\n",
    "            # Proposing New Trajectories\n",
    "            states[n], actions[n], rewards[n], logp[n], ancestors[n] = \\\n",
    "                kernel(env, policy, states[n-1], actions[n-1], rewards[n-1], logp[n-1] )\n",
    "            \n",
    "            # Weight Calculation\n",
    "            total_rewards = np.sum(rewards[n],axis=1) / horizon\n",
    "            total_rewards_ancestors = np.sum(rewards[n],axis=1) / horizon\n",
    "            \n",
    "            weights = ( total_rewards ** eta(n) / total_rewards_ancestors ** eta(n-1) ) \\\n",
    "                * np.exp(logp[n] - logp[n-1,ancestors[n-1].astype(int)])\n",
    "            \n",
    "            # Resampling\n",
    "            weights = weights / np.sum(weights)\n",
    "            selected = np.random.choice(range(sample_count), size=sample_count, p=weights, replace=True)\n",
    "\n",
    "            states[n] = states[n,selected]\n",
    "            actions[n] = actions[n,selected]\n",
    "            rewards[n] = rewards[n,selected]\n",
    "            logp[n] = logp[n,selected]\n",
    "            ancestors[n] = ancestors[n,selected]\n",
    "        \n",
    "    return states[-1], actions[-1], rewards[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequentially Interacting Markov Chain Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simcmc(env, policy, proposal=proposal_prior):\n",
    "    list(map(exec,[\"{0}={1}\".format(k,v) for k,v in parameters.items()]))\n",
    "    \n",
    "    states = np.zeros((temperature_count, sample_count, horizon, env.state_dimension))\n",
    "    actions = np.zeros((temperature_count, sample_count, horizon, env.action_dimension))\n",
    "    rewards = np.zeros((temperature_count, sample_count, horizon))\n",
    "    logp = np.zeros((temperature_count, sample_count))\n",
    "    weights_log = np.zeros((temperature_count, sample_count))\n",
    "\n",
    "    # Initial Trajectories\n",
    "    states[0], actions[0], rewards[0], logp[0] = \\\n",
    "        rollout_trajectories(env, policy, horizon, sample_count)\n",
    "\n",
    "    for s in range(sample_count):\n",
    "        for n in range(1,temperature_count):\n",
    "\n",
    "            # Sample Ancestor\n",
    "            a = np.random.randint(s+1)\n",
    "            # print(\"Sample {0!s} Brigde {1!s} : Ancestor <- {2!s}\".format(s,n,a))\n",
    "\n",
    "            # Sample Candidate \n",
    "            states_candidate, actions_candidate, rewards_candidate, logp_candidate = \\\n",
    "                proposal(states[n-1,a], actions[n-1,a], rewards[n-1,a], logp[n-1,a], policy)\n",
    "\n",
    "            # Calculate Weight of the Candidate\n",
    "            reward_candidate = np.sum(rewards_candidate) / horizon\n",
    "            reward_ancestor = np.sum(rewards[n-1,a,:]) / horizon\n",
    "            logp_ancestor = logp[n-1,a]\n",
    "            weights_log_candidate = np.log(reward_candidate)*eta(n) - np.log(reward_ancestor)*eta(n-1) - logp_ancestor\n",
    "\n",
    "            # Calculate Acceptance Rate\n",
    "            if s > 0:\n",
    "                acceptance = np.exp(weights_log_candidate - weights_log[n,s-1])\n",
    "            else:\n",
    "                acceptance = 1\n",
    "\n",
    "            # Accept or Reject\n",
    "            if np.random.rand() < acceptance:\n",
    "                states[n,s] = states_candidate\n",
    "                actions[n,s] = actions_candidate\n",
    "                rewards[n,s] = rewards_candidate\n",
    "                logp[n,s] = logp_candidate\n",
    "                weights_log[n,s] = weights_log_candidate\n",
    "            else:\n",
    "                states[n,s] = states[n,s-1]\n",
    "                actions[n,s] = actions[n,s-1]\n",
    "                rewards[n,s] = rewards[n,s-1]\n",
    "                logp[n,s] = logp[n,s-1]\n",
    "                weights_log[n,s] = weights_log[n,s-1]     \n",
    "         \n",
    "    return states[-1], actions[-1], rewards[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLEMMC Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "parameters['horizon'] = horizon\n",
    "parameters['iteration_count'] = iteration_count\n",
    "parameters['sample_count'] = sample_count\n",
    "parameters['burn_in'] = burn_in\n",
    "parameters['k_nearest'] = k_nearest\n",
    "parameters['policy_sample_count'] = policy_sample_count\n",
    "parameters['kernel'] = 'kernel_prior'\n",
    "parameters['proposal'] = 'proposal_prior_cached'\n",
    "parameters['temperatures'] = [0, 0.5, 1]\n",
    "parameters['temperature_count'] =  len(parameters['temperatures'])\n",
    "parameters['eta'] = 'eta'\n",
    "parameters['show_rewards'] = True\n",
    "parameters['experiment_count'] = 3\n",
    "parameters['test_variable'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rlemmc(env, inference, policy_approximation, parameters):\n",
    "    list(map(exec,[\"{0}={1}\".format(k,v) for k,v in parameters.items()]))\n",
    "    global cache_index\n",
    "    \n",
    "    iteration_rewards = []\n",
    "\n",
    "    for i in range(iteration_count):\n",
    "\n",
    "        if i == 0:\n",
    "            policy = UniformPolicyDiscrete(env.action_space)\n",
    "            states, actions, rewards, _ = \\\n",
    "                rollout_trajectories(env, policy, horizon, sample_count)\n",
    "            \n",
    "            # Initial Iteration Reward\n",
    "            iteration_rewards.append(np.mean(rewards) * horizon)\n",
    "            print( f'Iteration 0 - Average Reward : {iteration_rewards[-1]}' )\n",
    "\n",
    "        # E-Step\n",
    "        cache_index = -1 # Cache Reset for Proposal Functions\n",
    "        [states, actions, rewards] = inference(env, policy, parameters)\n",
    "\n",
    "        # M-Step\n",
    "        policy = policy_approximation.m_step(states, actions)\n",
    "\n",
    "        # Iteration Reward\n",
    "        iteration_rewards.append(np.mean(rewards) * horizon)\n",
    "        print( f'Iteration {i+1} - Average Reward : {iteration_rewards[-1]}' )\n",
    "    \n",
    "    return policy, iteration_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Importance Sampling\n",
      "Iteration 0 - Average Reward : 18.18\n",
      "Iteration 1 - Average Reward : 20.119999999999997\n",
      "Iteration 2 - Average Reward : 22.29\n",
      "Iteration 3 - Average Reward : 22.16\n",
      "Iteration 4 - Average Reward : 22.759999999999998\n",
      "Iteration 5 - Average Reward : 22.96\n",
      "Iteration 6 - Average Reward : 24.08\n",
      "Iteration 7 - Average Reward : 24.310000000000002\n",
      "Iteration 8 - Average Reward : 24.48\n",
      "Iteration 9 - Average Reward : 24.55\n",
      "Iteration 10 - Average Reward : 24.85\n",
      "\n",
      " Markov Chain Monte Carlo\n",
      "Iteration 0 - Average Reward : 18.459999999999997\n",
      "Iteration 1 - Average Reward : 20.880000000000003\n",
      "Iteration 2 - Average Reward : 23.7\n",
      "Iteration 3 - Average Reward : 24.13\n",
      "Iteration 4 - Average Reward : 24.490000000000002\n",
      "Iteration 5 - Average Reward : 24.52\n",
      "Iteration 6 - Average Reward : 24.79\n",
      "Iteration 7 - Average Reward : 24.959999999999997\n",
      "Iteration 8 - Average Reward : 24.91\n",
      "Iteration 9 - Average Reward : 24.9\n",
      "Iteration 10 - Average Reward : 25.0\n",
      "\n",
      " Sequential Monte Carlo Samplers\n",
      "Iteration 0 - Average Reward : 19.82\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kernel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0b9ab49df9b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n Sequential Monte Carlo Samplers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0moptimal_policy_smcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards_smcs\u001b[0m \u001b[1;33m=\u001b[0m     \u001b[0mrlemmc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_approximation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n Sequentially Interacting Markov Chain Monte Carlo'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-8ee910e5c4cf>\u001b[0m in \u001b[0;36mrlemmc\u001b[1;34m(env, inference, policy_approximation, parameters)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# E-Step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcache_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;31m# Cache Reset for Proposal Functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# M-Step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-9412cf83554a>\u001b[0m in \u001b[0;36msmcs\u001b[1;34m(env, policy, parameters)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;31m# Proposing New Trajectories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mancestors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m                 \u001b[0mkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# Weight Calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kernel' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\n Importance Sampling')\n",
    "optimal_policy_importance_sampling, rewards_importance_sampling = \\\n",
    "    rlemmc(env, importance_sampling, policy_approximation, parameters )\n",
    "\n",
    "print('\\n Markov Chain Monte Carlo')\n",
    "optimal_policy_mcmc, rewards_mcmc = \\\n",
    "    rlemmc(env, mcmc, policy_approximation, parameters )\n",
    "\n",
    "print('\\n Sequential Monte Carlo Samplers')\n",
    "optimal_policy_smcs, rewards_smcs = \\\n",
    "    rlemmc(env, smcs, policy_approximation, parameters )\n",
    "\n",
    "print('\\n Sequentially Interacting Markov Chain Monte Carlo')\n",
    "optimal_policy_smcs, rewards_simcmc = \\\n",
    "    rlemmc(env, simcmc, policy_approximation, parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sequential Monte Carlo Samplers\n",
      "Iteration 0 - Average Reward : 18.44\n",
      "{'horizon': 25, 'iteration_count': 10, 'sample_count': 100, 'burn_in': 10, 'k_nearest': 5, 'policy_sample_count': 100, 'kernel': 'kernel_prior', 'proposal': 'proposal_prior_cached', 'temperatures': [0, 0.5, 1], 'temperature_count': 3, 'eta': 'eta', 'show_rewards': True, 'experiment_count': 3}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kernel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-8e6a6badd55d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n Sequential Monte Carlo Samplers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moptimal_policy_smcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards_smcs\u001b[0m \u001b[1;33m=\u001b[0m     \u001b[0mrlemmc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_approximation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-8ee910e5c4cf>\u001b[0m in \u001b[0;36mrlemmc\u001b[1;34m(env, inference, policy_approximation, parameters)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# E-Step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcache_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;31m# Cache Reset for Proposal Functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# M-Step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-d1dfe268944f>\u001b[0m in \u001b[0;36msmcs\u001b[1;34m(env, policy, parameters)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemperature_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dimension\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kernel' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\n Sequential Monte Carlo Samplers')\n",
    "optimal_policy_smcs, rewards_smcs = \\\n",
    "    rlemmc(env, smcs, policy_approximation, parameters )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "plot_is = plt.plot(rewards_importance_sampling, label='Importance Sampling', color='black')\n",
    "patch_is = mpatches.Patch(color='black', label='Importance Sampling')\n",
    "\n",
    "plot_mcmc = plt.plot(rewards_mcmc, label='Markov Chain Monte Carlo', color='blue')\n",
    "patch_mcmc = mpatches.Patch(color='blue', label='Markov Chain Monte Carlo')\n",
    "\n",
    "plot_smcs = plt.plot(rewards_smcs, label='Sequential Monte Carlo Samplers', color='red')\n",
    "patch_smcs = mpatches.Patch(color='red', label='Sequential Monte Carlo Samplers')\n",
    "\n",
    "plot_simcmc = plt.plot(rewards_simcmc, label='Sequentially Interacting Markov Chain Monte Carlo', color='green')\n",
    "patch_simcmc = mpatches.Patch(color='green', label='Sequentially Interacting Markov Chain Monte Carlo')\n",
    "\n",
    "plt.hlines(np.min([horizon,195]), 0, iteration_count, linestyle='dotted')\n",
    "\n",
    "plt.legend(handles=[patch_is, patch_mcmc, patch_smcs, patch_simcmc], bbox_to_anchor=(0, 0, 1.9, 0.31))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLEMMC Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_experiment( experiment_rewards, label=None ):\n",
    "    \n",
    "    # Calculate Statistics\n",
    "    experiment_mean = np.mean(experiment_rewards,axis=0)\n",
    "    experiment_var = np.var(experiment_rewards,axis=0)\n",
    "    experiment_max = np.max(experiment_rewards,axis=0)\n",
    "    experiment_min = np.min(experiment_rewards,axis=0)\n",
    "    experiment_median = np.median(experiment_rewards,axis=0)\n",
    "    \n",
    "    # Mean Average Rewards\n",
    "    plt.plot(experiment_mean, color='#000099', linewidth=3)\n",
    "\n",
    "    # Min and Max Results\n",
    "    plt.plot(experiment_min, '--', color='black')\n",
    "    plt.plot(experiment_max, '--', color='black')\n",
    "    plt.fill_between(range(iteration_count+1), experiment_min, experiment_max, color='#ddeeff')\n",
    "\n",
    "    # Optimal or Exceptable Result\n",
    "    plt.hlines(np.min([horizon,195]), 0, iteration_count, linestyle='dotted', color='#333333')\n",
    "\n",
    "    # Axis Limits\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0,iteration_count])\n",
    "    # axes.set_ylim([0,horizon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(exec,[\"{0}={1}\".format(k,v) for k,v in parameters.items()]))\n",
    "\n",
    "inference_methods = [importance_sampling, mcmc, smcs, simcmc]\n",
    "inference_names = ['IS','MCMC', 'SMCS', 'SIMCMC']\n",
    "\n",
    "for m in range(len(inference_methods)):\n",
    "    print(inference_names[m])\n",
    "    \n",
    "    experiment_rewards = np.zeros((experiment_count, iteration_count+1))\n",
    "    \n",
    "    for e in range(experiment_count):\n",
    "        print(\"...Run \" + str(e))\n",
    "    \n",
    "        _, experiment_rewards[e] = rlemmc(env, inference_methods[m], policy_approximation, parameters )\n",
    "        \n",
    "    plot_experiment(experiment_rewards, label=inference_methods[m])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
