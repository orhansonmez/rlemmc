{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Cart Pole\n",
    "\n",
    "This is the noisy version of the CartPole-v0 environment of OpenAI.  \n",
    "https://gym.openai.com/envs/CartPole-v0  \n",
    "https://github.com/openai/gym/wiki/CartPole-v0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NoisyCartPole:\n",
    "\n",
    "    stateDimension = 4\n",
    "    actionDimension = 1\n",
    "    transitionSigmas = [ 0.01, 0, 0.01, 0 ]\n",
    "    alreadyFinished = False\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.openAI = gym.make('CartPole-v0')\n",
    "        \n",
    "    def reset(self):\n",
    "        self.alreadyFinished = False\n",
    "        return self.openAI.reset()\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \n",
    "        if self.alreadyFinished:\n",
    "            # Zero reward till the end of the episode if pole is already dropped\n",
    "            return state, 0, True, None\n",
    "        \n",
    "        else:\n",
    "            # OpenAI Step\n",
    "            next_state, reward, self.alreadyFinished, info = self.openAI.step(int(action))\n",
    "            # Adding Noise \n",
    "            next_state += np.random.randn(self.stateDimension) * self.transitionSigmas\n",
    "            \n",
    "            return next_state, reward, self.alreadyFinished, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cartpole_reset():\n",
    "    state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "    return np.array(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracted from OpenAI environment CartPole-v0\n",
    "def cartpole_step(state, action):\n",
    "    \n",
    "    gravity = 9.8\n",
    "    masscart = 1.0\n",
    "    masspole = 0.1\n",
    "    total_mass = (masspole + masscart)\n",
    "    length = 0.5 # actually half the pole's length\n",
    "    polemass_length = (masspole * length)\n",
    "    force_mag = 10.0\n",
    "    tau = 0.02  # seconds between state updates\n",
    "\n",
    "    # Angle at which to fail the episode\n",
    "    theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "    x_threshold = 2.4\n",
    "    \n",
    "    x, x_dot, theta, theta_dot = state\n",
    "    \n",
    "    already_done =  x < -x_threshold \\\n",
    "        or x > x_threshold \\\n",
    "        or theta < -theta_threshold_radians \\\n",
    "        or theta > theta_threshold_radians\n",
    "    already_done = bool(already_done)\n",
    "    \n",
    "    if already_done:\n",
    "        \n",
    "        next_state = state\n",
    "        reward = 0\n",
    "        done = True\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        force = force_mag if action==1 else -force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        temp = (force + polemass_length * theta_dot * theta_dot * sintheta) / total_mass\n",
    "        thetaacc = (gravity * sintheta - costheta* temp) / (length * (4.0/3.0 - masspole * costheta * costheta / total_mass))\n",
    "        xacc  = temp - polemass_length * thetaacc * costheta / total_mass\n",
    "        x  = x + tau * x_dot\n",
    "        x_dot = x_dot + tau * xacc\n",
    "        theta = theta + tau * theta_dot\n",
    "        theta_dot = theta_dot + tau * thetaacc\n",
    "        next_state = np.array([x,x_dot,theta,theta_dot])\n",
    "        \n",
    "        reward = 1\n",
    "        \n",
    "        done =  x < -x_threshold \\\n",
    "            or x > x_threshold \\\n",
    "            or theta < -theta_threshold_radians \\\n",
    "            or theta > theta_threshold_radians\n",
    "        done = bool(done)\n",
    "    \n",
    "    return next_state, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisycartpole_reset():\n",
    "    return cartpole_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisycartpole_step(state, action):\n",
    "        \n",
    "    next_state, reward, done, info = cartpole_step(state, action)   # CartPole Step\n",
    "    \n",
    "    if not done:\n",
    "        next_state += np.random.randn(4) * [ 0.01, 0, 0.01, 0 ]         # Adding Noise \n",
    "\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5ee279840b55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Environment Properties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateDimension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionDimension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactionSpace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnoisycartpole_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# Environment Properties\n",
    "env = {}\n",
    "env.stateDimension = 4\n",
    "env.actionDimension = 1\n",
    "env.actionSpace = range(2)\n",
    "env.step = noisycartpole_step\n",
    "env.reset = noisycartpole_reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trajectory2tuples(states, actions):\n",
    "\n",
    "    # Dimensions\n",
    "    [sample_count, horizon, state_dimension] = states.shape\n",
    "    [_, _, action_dimension] = actions.shape\n",
    "\n",
    "    # Reshape Inputs and Targets\n",
    "    inputs = np.reshape(states, (sample_count*horizon, state_dimension))\n",
    "    targets = np.reshape(actions, (sample_count*horizon, action_dimension))\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(states, color='red', n=0):\n",
    "\n",
    "    [sample_count, _, _] = states.shape\n",
    "\n",
    "    if n==0:\n",
    "        samples_drawn = range(sample_count)\n",
    "    else:\n",
    "        samples_drawn = np.random.choice(sample_count, n)\n",
    "        \n",
    "    for s in samples_drawn:\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], '-', color=color)\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], 'o', color=color, markersize=2)\n",
    "        plt.plot(states[s, -1, 0], states[s, -1, 2], 'o', color=color, markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_iteration(states, selected=None, n=0):\n",
    "    plot_trajectories(states, color='red', n=n)\n",
    "    if selected is not None:\n",
    "        plot_trajectories(selected, color='green', n=n)\n",
    "    \n",
    "    plt.vlines(0, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(-2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    \n",
    "    plt.hlines(0, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(-0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rollout_trajectories(env, policy, horizon, sample_count=1):\n",
    "\n",
    "    # States and Actions\n",
    "    states = np.zeros((sample_count, horizon, env.stateDimension))\n",
    "    actions = np.zeros((sample_count, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((sample_count, horizon))\n",
    "    \n",
    "    # Sample Trajectories\n",
    "    for t in range(horizon):\n",
    "\n",
    "        # Initialization\n",
    "        if t == 0:\n",
    "            states[:,t,:] = [ env.reset() for i in range(sample_count) ]\n",
    "        # Transition and Reward\n",
    "        else:\n",
    "            for s in range(sample_count):\n",
    "                states[s, t, :], rewards[s,t-1], _1, _2 = env.step(states[s, t-1, :], actions[s, t-1, :])\n",
    "        \n",
    "        actions[:,t,:] = iterationPolicy.query(states[:, t, :]).reshape(sample_count, env.actionDimension)\n",
    "        \n",
    "    for s in range(sample_count):\n",
    "        _, rewards[s, horizon-1], _1, _2 = env.step(states[s, horizon-1, :], actions[s, horizon-1, :])\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Sampling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def importance_sampling(states, actions, rewards, policy_sample_count=0):\n",
    "\n",
    "    # Dimensions\n",
    "    [sample_count, horizon, state_dimension] = states.shape\n",
    "    [_, _, action_dimension] = actions.shape\n",
    "\n",
    "    if policy_sample_count <= 0:\n",
    "        policy_sample_count = sample_count\n",
    "\n",
    "    # Weighting\n",
    "    if sum(rewards) == 0:\n",
    "        weights = np.ones(sample_count) / sample_count\n",
    "    else:\n",
    "        weights = rewards / sum(rewards)\n",
    "\n",
    "    # Resampling\n",
    "    index = np.random.choice(range(sample_count), size=policy_sample_count, p=weights, replace=True)\n",
    "\n",
    "    # New Trajectories\n",
    "    states_new = np.zeros((policy_sample_count, horizon, state_dimension))\n",
    "    actions_new = np.zeros((policy_sample_count, horizon, action_dimension))\n",
    "    for s in range(policy_sample_count):\n",
    "        states_new[s] = states[index[s], :, :]\n",
    "        actions_new[s] = actions[index[s], :, :]\n",
    "\n",
    "    return [states_new, actions_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SciKitPolicy():\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def query(self, states):\n",
    "        if len(states.shape) == 1:\n",
    "            states = states.reshape(1, -1)\n",
    "        return self.method.predict(states)\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        self.method.fit(inputs, targets)\n",
    "\n",
    "    def m_step(self, states, actions):\n",
    "\n",
    "        # States/Actions -> Inputs/Targets\n",
    "        inputs, targets = trajectory2tuples(states, actions)\n",
    "\n",
    "        # Train kNN\n",
    "        self.train(inputs, targets.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KnnPolicyDiscrete(SciKitPolicy):\n",
    "    def __init__(self, k, weights='distance'):\n",
    "        self.method = KNeighborsClassifier(n_neighbors=k, weights=weights, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UniformPolicyDiscrete():\n",
    "\n",
    "    def __init__(self, choices):\n",
    "        self.choices = choices\n",
    "\n",
    "    def query(self, states):\n",
    "        return np.random.choice(self.choices, size=states.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Environment  \n",
    "horizon = 100\n",
    "\n",
    "# Inference\n",
    "sampleCount = 100\n",
    "iterationCount = 50\n",
    "selectedInference = importance_sampling\n",
    "\n",
    "# Policy\n",
    "kNearest = 5\n",
    "selectedPolicy = KnnPolicyDiscrete(kNearest)\n",
    "\n",
    "# Plot\n",
    "rendering_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLEMMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterationRewards = []\n",
    "\n",
    "for i in range(iterationCount):\n",
    "\n",
    "    if i == 0:\n",
    "        iterationPolicy = UniformPolicyDiscrete(actionSpace)\n",
    "    else:\n",
    "        iterationPolicy = selectedPolicy\n",
    "\n",
    "    [states, actions, rewards] = \\\n",
    "        rollout_trajectories(env, iterationPolicy, horizon, sampleCount)\n",
    "\n",
    "    # E-Step\n",
    "    [newStates, newActions] = selectedInference(states, actions, np.sum(rewards,axis=1))\n",
    "\n",
    "    # M-Step\n",
    "    selectedPolicy.m_step(newStates, newActions)\n",
    "        \n",
    "    # Average Reward\n",
    "    iterationRewards.append(np.mean(rewards) * horizon)\n",
    "    print( f'Iteration {i+1} - Average Reward : {iterationRewards[i]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(iterationRewards)\n",
    "plt.hlines(np.min([horizon,195]), 0, iterationCount, linestyle='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_iteration(states, n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "def render_state(env, t):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Timestep : %s\" %(env.spec.id, t))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if rendering_enabled:\n",
    "    env = gym.make('CartPole-v0')\n",
    "\n",
    "    state = env.reset()\n",
    "    for t in range(horizon):\n",
    "        render_state(env,t)\n",
    "        state,_,done,_ = env.step(int(iterationPolicy.query(state)))\n",
    "        if done:\n",
    "            break        \n",
    "    render_state(env,t)\n",
    "\n",
    "    env.render(close=True)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
