{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Cart Pole\n",
    "\n",
    "This is the noisy version of the CartPole-v0 environment of OpenAI.  \n",
    "https://gym.openai.com/envs/CartPole-v0  \n",
    "https://github.com/openai/gym/wiki/CartPole-v0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NoisyCartPoleEnvironment:\n",
    "    \n",
    "    stateDimension = 4\n",
    "    actionDimension = 1\n",
    "    actionSpace = range(2)\n",
    "    transitionSigmas = [ 1e-2, 1e-5, 1e-2, 1e-5 ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cartpole_reset(self):\n",
    "        state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        return np.array(state)\n",
    "    \n",
    "    # Extracted from OpenAI environment CartPole-v0\n",
    "    def cartpole_step(self, state, action):\n",
    "\n",
    "        gravity = 9.8\n",
    "        masscart = 1.0\n",
    "        masspole = 0.1\n",
    "        total_mass = (masspole + masscart)\n",
    "        length = 0.5 # actually half the pole's length\n",
    "        polemass_length = (masspole * length)\n",
    "        force_mag = 10.0\n",
    "        tau = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        x_threshold = 2.4\n",
    "\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "\n",
    "        already_done =  x < -x_threshold \\\n",
    "            or x > x_threshold \\\n",
    "            or theta < -theta_threshold_radians \\\n",
    "            or theta > theta_threshold_radians\n",
    "        already_done = bool(already_done)\n",
    "\n",
    "        if already_done:\n",
    "\n",
    "            next_state = state\n",
    "            reward = 0\n",
    "            done = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            force = force_mag if action==1 else -force_mag\n",
    "            costheta = math.cos(theta)\n",
    "            sintheta = math.sin(theta)\n",
    "            temp = (force + polemass_length * theta_dot * theta_dot * sintheta) / total_mass\n",
    "            thetaacc = (gravity * sintheta - costheta* temp) / (length * (4.0/3.0 - masspole * costheta * costheta / total_mass))\n",
    "            xacc  = temp - polemass_length * thetaacc * costheta / total_mass\n",
    "            x  = x + tau * x_dot\n",
    "            x_dot = x_dot + tau * xacc\n",
    "            theta = theta + tau * theta_dot\n",
    "            theta_dot = theta_dot + tau * thetaacc\n",
    "            next_state = np.array([x,x_dot,theta,theta_dot])\n",
    "\n",
    "            reward = 1\n",
    "\n",
    "            done =  x < -x_threshold \\\n",
    "                or x > x_threshold \\\n",
    "                or theta < -theta_threshold_radians \\\n",
    "                or theta > theta_threshold_radians\n",
    "            done = bool(done)\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def noisycartpole_reset(self):\n",
    "        return self.cartpole_reset()\n",
    "\n",
    "    def noisycartpole_step(self, state, action):\n",
    "\n",
    "        next_state_mean, reward, done, info = self.cartpole_step(state, action)   # CartPole Step\n",
    "\n",
    "        noise = np.zeros(self.stateDimension)\n",
    "        if not done:\n",
    "            noise = np.random.randn(self.stateDimension) * self.transitionSigmas        # Adding Noise\n",
    "        next_state = next_state_mean + noise\n",
    "\n",
    "        # logp = multivariate_normal.logpdf(next_state, mean=next_state_mean, cov=np.diagflat(self.transitionSigmas))\n",
    "\n",
    "        return next_state, reward, done, None\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.noisycartpole_reset()\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        return self.noisycartpole_step(state, action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = NoisyCartPoleEnvironment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trajectory2tuples(states, actions):\n",
    "\n",
    "    # Dimensions\n",
    "    [sample_count, horizon, state_dimension] = states.shape\n",
    "    [_, _, action_dimension] = actions.shape\n",
    "\n",
    "    # Reshape Inputs and Targets\n",
    "    inputs = np.reshape(states, (sample_count*horizon, state_dimension))\n",
    "    targets = np.reshape(actions, (sample_count*horizon, action_dimension))\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(states, color='red', n=0):\n",
    "\n",
    "    [sample_count, _, _] = states.shape\n",
    "\n",
    "    if n==0:\n",
    "        samples_drawn = range(sample_count)\n",
    "    else:\n",
    "        samples_drawn = np.random.choice(sample_count, n)\n",
    "        \n",
    "    for s in samples_drawn:\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], '-', color=color)\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], 'o', color=color, markersize=2)\n",
    "        plt.plot(states[s, -1, 0], states[s, -1, 2], 'o', color=color, markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_iteration(states, selected=None, n=0):\n",
    "    plot_trajectories(states, color='red', n=n)\n",
    "    if selected is not None:\n",
    "        plot_trajectories(selected, color='green', n=n)\n",
    "    \n",
    "    plt.vlines(0, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(-2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    \n",
    "    plt.hlines(0, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(-0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rollout_trajectories(env, policy, horizon, sample_count=1, init=None):\n",
    "\n",
    "    # States and Actions\n",
    "    states = np.zeros((sample_count, horizon, env.stateDimension))\n",
    "    actions = np.zeros((sample_count, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((sample_count, horizon))\n",
    "    \n",
    "    # Sample Trajectories\n",
    "    for t in range(horizon):\n",
    "\n",
    "        # Initialization\n",
    "        if t == 0:\n",
    "            if init is None:\n",
    "                states[:,t,:] = [ env.reset() for i in range(sample_count) ]\n",
    "            else:\n",
    "                states[:,t,:] = init\n",
    "                \n",
    "        # Transition and Reward\n",
    "        else:\n",
    "            for s in range(sample_count):\n",
    "                states[s, t, :], rewards[s,t-1], _1, _2 = env.step(states[s, t-1, :], actions[s, t-1, :])\n",
    "        \n",
    "        actions[:,t,:] = policy.query(states[:, t, :]).reshape(sample_count, env.actionDimension)\n",
    "        \n",
    "    for s in range(sample_count):\n",
    "        _, rewards[s, horizon-1], _1, _2 = env.step(states[s, horizon-1, :], actions[s, horizon-1, :])\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Sampling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proposal_prior(states_previous, actions_previous, rewards_previous, policy):\n",
    "    return rollout_trajectories(env, policy, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proposal_prior_cached(_1, _2, _3, policy):\n",
    "    global states_cached, actions_cached, rewards_cached, cache_index\n",
    "    \n",
    "    if np.sum(states_cached) == 0:\n",
    "        states_cached, actions_cached, rewards_cached = \\\n",
    "            rollout_trajectories(env, policy, horizon, sample_count=sample_count + burn_in)\n",
    "    \n",
    "    cache_index +=1\n",
    "    \n",
    "    return states_cached[cache_index,:,:], actions_cached[cache_index,:,:], rewards_cached[cache_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proposal_keepthenprior(states, actions, rewards, policy):\n",
    "    \n",
    "    t = np.random.randint(states.shape[0]+1) -1\n",
    "    \n",
    "    if t < 0:\n",
    "        states, actions, rewards = \\\n",
    "            rollout_trajectories(env, policy, horizon )\n",
    "    else:\n",
    "        states[t:,:], actions[t:,:], rewards[t:] = \\\n",
    "            rollout_trajectories(env, policy, horizon-t, init=states[t])\n",
    "    \n",
    "    return states, actions, rewards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mcmc(env, policy):\n",
    "    \n",
    "    # States and Actions\n",
    "    states = np.zeros((sample_count + burn_in, horizon, env.stateDimension))\n",
    "    actions = np.zeros((sample_count + burn_in, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((sample_count + burn_in, horizon))\n",
    "\n",
    "    for i in range(sample_count + burn_in):\n",
    "        \n",
    "        if i == 0:\n",
    "            states[i,:,:], actions[i,:,:], rewards[i,:] = \\\n",
    "                rollout_trajectories(env, policy, horizon)\n",
    "        else:\n",
    "            states_candidate, actions_candidate, rewards_candidate = \\\n",
    "                proposal_prior_cached(states[i-1,:,:], actions[i-1,:,:], rewards[i-1,:], policy)\n",
    "            \n",
    "            reward_previous = np.sum(rewards[i-1,:])    \n",
    "            reward_candidate = np.sum(rewards_candidate)\n",
    "\n",
    "            alpha = reward_candidate / reward_previous\n",
    "\n",
    "            if np.random.rand() < alpha:\n",
    "                states[i,:,:] = states_candidate\n",
    "                actions[i,:,:] = actions_candidate\n",
    "                rewards[i,:] = rewards_candidate\n",
    "            else:\n",
    "                states[i,:,:] = states[i-1,:,:]\n",
    "                actions[i,:,:] = actions[i-1,:,:]\n",
    "                rewards[i,:] = rewards[i-1,:]\n",
    "        \n",
    "        if rendering_enabled:\n",
    "            if i < running_average:\n",
    "                plot_mcmc(states[0:i,:,:], rewards[0:i,:], i)\n",
    "            else:\n",
    "                plot_mcmc(states[i-running_average+1:i,:,:], rewards[i-running_average+1:i,:], i)\n",
    "        \n",
    "         # Resampling\n",
    "        index = np.random.choice(range(sample_count), size=policy_sample_count, replace=False)\n",
    "\n",
    "        # Policy Trajectories\n",
    "        states_new = np.zeros((policy_sample_count, horizon, env.stateDimension))\n",
    "        actions_new = np.zeros((policy_sample_count, horizon, env.actionDimension))\n",
    "        rewards_new = np.zeros((policy_sample_count, horizon))\n",
    "        for s in range(policy_sample_count):\n",
    "            states_new[s] = states[index[s] + burn_in, :, :]\n",
    "            actions_new[s] = actions[index[s] + burn_in, :, :]\n",
    "            rewards_new[s] = rewards[index[s] + burn_in, : ]\n",
    "        \n",
    "    return states_new, actions_new, rewards_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SciKitPolicy():\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def query(self, states):\n",
    "        if len(states.shape) == 1:\n",
    "            states = states.reshape(1, -1)\n",
    "        return self.method.predict(states)\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        self.method.fit(inputs, targets)\n",
    "\n",
    "    def m_step(self, states, actions):\n",
    "\n",
    "        # States/Actions -> Inputs/Targets\n",
    "        inputs, targets = trajectory2tuples(states, actions)\n",
    "\n",
    "        # Train kNN\n",
    "        self.train(inputs, targets.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KnnPolicyDiscrete(SciKitPolicy):\n",
    "    def __init__(self, k, weights='distance'):\n",
    "        self.method = KNeighborsClassifier(n_neighbors=k, weights=weights, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UniformPolicyDiscrete():\n",
    "\n",
    "    def __init__(self, choices):\n",
    "        self.choices = choices\n",
    "\n",
    "    def query(self, states):\n",
    "        return np.random.choice(self.choices, size=states.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Environment  \n",
    "horizon = 200\n",
    "\n",
    "# Inference\n",
    "iteration_count = 200\n",
    "sample_count = 1000\n",
    "burn_in = 100\n",
    "\n",
    "# Policy\n",
    "kNearest = 5\n",
    "policy_sample_count = 200\n",
    "selectedPolicy = KnnPolicyDiscrete(kNearest)\n",
    "\n",
    "# Plot\n",
    "rendering_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLEMMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Average Reward : 25.145\n",
      "Iteration 2 - Average Reward : 34.02\n",
      "Iteration 3 - Average Reward : 41.13\n",
      "Iteration 4 - Average Reward : 63.615\n",
      "Iteration 5 - Average Reward : 69.26\n",
      "Iteration 6 - Average Reward : 85.085\n",
      "Iteration 7 - Average Reward : 85.85000000000001\n",
      "Iteration 8 - Average Reward : 92.07\n",
      "Iteration 9 - Average Reward : 104.71\n",
      "Iteration 10 - Average Reward : 108.175\n",
      "Iteration 11 - Average Reward : 122.22500000000001\n",
      "Iteration 12 - Average Reward : 116.27499999999999\n",
      "Iteration 13 - Average Reward : 129.56\n",
      "Iteration 14 - Average Reward : 132.695\n",
      "Iteration 15 - Average Reward : 132.39000000000001\n",
      "Iteration 16 - Average Reward : 137.23499999999999\n",
      "Iteration 17 - Average Reward : 129.215\n",
      "Iteration 18 - Average Reward : 130.655\n",
      "Iteration 19 - Average Reward : 127.855\n",
      "Iteration 20 - Average Reward : 131.01\n",
      "Iteration 21 - Average Reward : 138.935\n",
      "Iteration 22 - Average Reward : 142.815\n",
      "Iteration 23 - Average Reward : 137.93\n",
      "Iteration 24 - Average Reward : 141.35999999999999\n",
      "Iteration 25 - Average Reward : 137.66\n",
      "Iteration 26 - Average Reward : 140.01999999999998\n",
      "Iteration 27 - Average Reward : 139.04000000000002\n",
      "Iteration 28 - Average Reward : 134.85\n",
      "Iteration 29 - Average Reward : 133.45\n",
      "Iteration 30 - Average Reward : 140.42\n",
      "Iteration 31 - Average Reward : 135.495\n",
      "Iteration 32 - Average Reward : 143.245\n",
      "Iteration 33 - Average Reward : 140.995\n",
      "Iteration 34 - Average Reward : 135.38\n",
      "Iteration 35 - Average Reward : 144.47\n",
      "Iteration 36 - Average Reward : 139.92\n",
      "Iteration 37 - Average Reward : 141.185\n",
      "Iteration 38 - Average Reward : 135.45499999999998\n",
      "Iteration 39 - Average Reward : 132.95999999999998\n",
      "Iteration 40 - Average Reward : 137.35\n",
      "Iteration 41 - Average Reward : 144.31\n",
      "Iteration 42 - Average Reward : 140.09\n",
      "Iteration 43 - Average Reward : 138.74\n",
      "Iteration 44 - Average Reward : 147.15\n",
      "Iteration 45 - Average Reward : 137.32999999999998\n",
      "Iteration 46 - Average Reward : 140.025\n",
      "Iteration 47 - Average Reward : 144.67000000000002\n",
      "Iteration 48 - Average Reward : 141.73\n",
      "Iteration 49 - Average Reward : 139.35500000000002\n",
      "Iteration 50 - Average Reward : 136.82000000000002\n",
      "Iteration 51 - Average Reward : 137.86499999999998\n",
      "Iteration 52 - Average Reward : 137.595\n",
      "Iteration 53 - Average Reward : 131.78\n",
      "Iteration 54 - Average Reward : 133.93\n",
      "Iteration 55 - Average Reward : 134.525\n",
      "Iteration 56 - Average Reward : 139.375\n",
      "Iteration 57 - Average Reward : 137.38\n",
      "Iteration 58 - Average Reward : 136.08499999999998\n",
      "Iteration 59 - Average Reward : 133.32500000000002\n",
      "Iteration 60 - Average Reward : 134.10999999999999\n",
      "Iteration 61 - Average Reward : 135.435\n",
      "Iteration 62 - Average Reward : 137.76\n",
      "Iteration 63 - Average Reward : 141.845\n",
      "Iteration 64 - Average Reward : 138.89000000000001\n",
      "Iteration 65 - Average Reward : 133.97\n",
      "Iteration 66 - Average Reward : 135.935\n",
      "Iteration 67 - Average Reward : 129.48999999999998\n",
      "Iteration 68 - Average Reward : 125.875\n",
      "Iteration 69 - Average Reward : 131.70999999999998\n",
      "Iteration 70 - Average Reward : 128.015\n",
      "Iteration 71 - Average Reward : 127.39\n",
      "Iteration 72 - Average Reward : 127.435\n",
      "Iteration 73 - Average Reward : 122.35000000000001\n",
      "Iteration 74 - Average Reward : 129.005\n",
      "Iteration 75 - Average Reward : 137.965\n",
      "Iteration 76 - Average Reward : 131.595\n",
      "Iteration 77 - Average Reward : 127.35499999999999\n",
      "Iteration 78 - Average Reward : 131.42000000000002\n",
      "Iteration 79 - Average Reward : 126.55000000000001\n",
      "Iteration 80 - Average Reward : 133.9\n",
      "Iteration 81 - Average Reward : 126.88\n",
      "Iteration 82 - Average Reward : 126.955\n",
      "Iteration 83 - Average Reward : 126.285\n",
      "Iteration 84 - Average Reward : 125.93\n",
      "Iteration 85 - Average Reward : 124.07999999999998\n",
      "Iteration 86 - Average Reward : 127.64\n",
      "Iteration 87 - Average Reward : 127.16499999999999\n",
      "Iteration 88 - Average Reward : 127.745\n",
      "Iteration 89 - Average Reward : 127.58\n",
      "Iteration 90 - Average Reward : 122.55\n",
      "Iteration 91 - Average Reward : 126.77000000000001\n",
      "Iteration 92 - Average Reward : 124.28999999999999\n",
      "Iteration 93 - Average Reward : 128.53\n",
      "Iteration 94 - Average Reward : 122.015\n",
      "Iteration 95 - Average Reward : 128.945\n",
      "Iteration 96 - Average Reward : 125.465\n",
      "Iteration 97 - Average Reward : 113.96\n",
      "Iteration 98 - Average Reward : 118.96\n",
      "Iteration 99 - Average Reward : 114.695\n",
      "Iteration 100 - Average Reward : 119.285\n",
      "Iteration 101 - Average Reward : 119.38\n",
      "Iteration 102 - Average Reward : 117.75\n",
      "Iteration 103 - Average Reward : 116.865\n",
      "Iteration 104 - Average Reward : 112.435\n",
      "Iteration 105 - Average Reward : 117.335\n",
      "Iteration 106 - Average Reward : 114.32500000000002\n",
      "Iteration 107 - Average Reward : 115.745\n",
      "Iteration 108 - Average Reward : 114.38\n",
      "Iteration 109 - Average Reward : 113.32\n",
      "Iteration 110 - Average Reward : 114.43\n",
      "Iteration 111 - Average Reward : 110.625\n",
      "Iteration 112 - Average Reward : 110.645\n",
      "Iteration 113 - Average Reward : 112.155\n",
      "Iteration 114 - Average Reward : 113.97999999999999\n",
      "Iteration 115 - Average Reward : 115.14500000000001\n",
      "Iteration 116 - Average Reward : 119.00500000000001\n",
      "Iteration 117 - Average Reward : 118.36\n",
      "Iteration 118 - Average Reward : 115.82\n",
      "Iteration 119 - Average Reward : 114.77999999999999\n",
      "Iteration 120 - Average Reward : 115.29\n",
      "Iteration 121 - Average Reward : 113.41499999999999\n",
      "Iteration 122 - Average Reward : 115.015\n",
      "Iteration 123 - Average Reward : 112.455\n",
      "Iteration 124 - Average Reward : 105.80000000000001\n",
      "Iteration 125 - Average Reward : 111.77\n",
      "Iteration 126 - Average Reward : 114.995\n",
      "Iteration 127 - Average Reward : 115.245\n",
      "Iteration 128 - Average Reward : 117.25000000000001\n",
      "Iteration 129 - Average Reward : 109.48\n",
      "Iteration 130 - Average Reward : 108.555\n",
      "Iteration 131 - Average Reward : 110.01\n",
      "Iteration 132 - Average Reward : 112.715\n",
      "Iteration 133 - Average Reward : 114.07000000000001\n",
      "Iteration 134 - Average Reward : 116.445\n",
      "Iteration 135 - Average Reward : 115.33500000000001\n",
      "Iteration 136 - Average Reward : 114.03000000000002\n",
      "Iteration 137 - Average Reward : 117.405\n",
      "Iteration 138 - Average Reward : 110.38\n",
      "Iteration 139 - Average Reward : 122.49000000000001\n",
      "Iteration 140 - Average Reward : 109.71\n",
      "Iteration 141 - Average Reward : 112.36999999999999\n",
      "Iteration 142 - Average Reward : 110.97999999999999\n",
      "Iteration 143 - Average Reward : 111.195\n",
      "Iteration 144 - Average Reward : 115.14\n",
      "Iteration 145 - Average Reward : 113.68\n",
      "Iteration 146 - Average Reward : 117.085\n",
      "Iteration 147 - Average Reward : 113.41000000000001\n",
      "Iteration 148 - Average Reward : 116.8\n",
      "Iteration 149 - Average Reward : 114.385\n",
      "Iteration 150 - Average Reward : 107.74999999999999\n",
      "Iteration 151 - Average Reward : 112.04499999999999\n",
      "Iteration 152 - Average Reward : 114.58500000000001\n",
      "Iteration 153 - Average Reward : 106.84\n",
      "Iteration 154 - Average Reward : 106.145\n",
      "Iteration 155 - Average Reward : 115.87\n",
      "Iteration 156 - Average Reward : 108.355\n",
      "Iteration 157 - Average Reward : 114.09\n",
      "Iteration 158 - Average Reward : 114.555\n",
      "Iteration 159 - Average Reward : 111.04499999999999\n",
      "Iteration 160 - Average Reward : 108.38499999999999\n",
      "Iteration 161 - Average Reward : 108.495\n",
      "Iteration 162 - Average Reward : 107.08\n",
      "Iteration 163 - Average Reward : 106.79\n",
      "Iteration 164 - Average Reward : 110.07000000000001\n",
      "Iteration 165 - Average Reward : 106.44\n",
      "Iteration 166 - Average Reward : 111.785\n",
      "Iteration 167 - Average Reward : 103.54\n",
      "Iteration 168 - Average Reward : 108.66499999999999\n",
      "Iteration 169 - Average Reward : 106.30999999999999\n",
      "Iteration 170 - Average Reward : 105.095\n",
      "Iteration 171 - Average Reward : 104.21499999999999\n",
      "Iteration 172 - Average Reward : 105.345\n",
      "Iteration 173 - Average Reward : 104.08\n",
      "Iteration 174 - Average Reward : 107.375\n",
      "Iteration 175 - Average Reward : 106.345\n",
      "Iteration 176 - Average Reward : 103.42\n",
      "Iteration 177 - Average Reward : 109.14500000000001\n",
      "Iteration 178 - Average Reward : 105.035\n",
      "Iteration 179 - Average Reward : 100.74499999999999\n",
      "Iteration 180 - Average Reward : 106.61\n",
      "Iteration 181 - Average Reward : 104.16499999999999\n",
      "Iteration 182 - Average Reward : 102.06500000000001\n",
      "Iteration 183 - Average Reward : 103.32\n",
      "Iteration 184 - Average Reward : 100.2\n",
      "Iteration 185 - Average Reward : 101.81\n",
      "Iteration 186 - Average Reward : 103.06500000000001\n",
      "Iteration 187 - Average Reward : 104.69\n",
      "Iteration 188 - Average Reward : 102.60000000000001\n",
      "Iteration 189 - Average Reward : 101.08\n",
      "Iteration 190 - Average Reward : 106.06500000000001\n",
      "Iteration 191 - Average Reward : 100.58\n",
      "Iteration 192 - Average Reward : 100.05\n",
      "Iteration 193 - Average Reward : 105.04500000000002\n",
      "Iteration 194 - Average Reward : 102.64500000000001\n",
      "Iteration 195 - Average Reward : 109.715\n"
     ]
    }
   ],
   "source": [
    "iteration_rewards = []\n",
    "\n",
    "for i in range(iteration_count):\n",
    "    \n",
    "    if i == 0:\n",
    "        iteration_policy = UniformPolicyDiscrete(env.actionSpace)\n",
    "    else:\n",
    "        iteration_policy = selectedPolicy\n",
    "    \n",
    "    # Cache for Proposal Function\n",
    "    cache_index = -1\n",
    "    states_cached = np.zeros((sample_count + burn_in, horizon, env.stateDimension))\n",
    "    actions_cached = np.zeros((sample_count + burn_in, horizon, env.actionDimension))\n",
    "    rewards_cached = np.zeros((sample_count + burn_in, horizon))\n",
    "\n",
    "    # E-Step\n",
    "    [states, actions, rewards] = mcmc(env, iteration_policy)\n",
    "\n",
    "    # M-Step\n",
    "    selectedPolicy.m_step(states, actions)\n",
    "        \n",
    "    # Average Reward\n",
    "    iteration_rewards.append(np.mean(rewards) * horizon)\n",
    "    print( f'Iteration {i+1} - Average Reward : {iteration_rewards[i]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(iteration_rewards)\n",
    "plt.hlines(np.min([horizon,195]), 0, iteration_count, linestyle='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_iteration(states, n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "def render_state(env, t):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Timestep : %s\" %(env.spec.id, t))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if rendering_enabled:\n",
    "    env = gym.make('CartPole-v0')\n",
    "\n",
    "    state = env.reset()\n",
    "    for t in range(horizon):\n",
    "        render_state(env,t)\n",
    "        state,_,done,_ = env.step(int(iteration_policy.query(state)))\n",
    "        if done:\n",
    "            break        \n",
    "    render_state(env,t)\n",
    "\n",
    "    env.render(close=True)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
