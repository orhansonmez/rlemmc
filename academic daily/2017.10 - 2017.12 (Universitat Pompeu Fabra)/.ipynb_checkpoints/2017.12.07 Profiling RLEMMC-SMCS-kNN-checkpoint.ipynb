{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Cart Pole\n",
    "\n",
    "This is the noisy version of the CartPole-v0 environment of OpenAI.  \n",
    "https://gym.openai.com/envs/CartPole-v0  \n",
    "https://github.com/openai/gym/wiki/CartPole-v0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoisyCartPoleEnvironment:\n",
    "    \n",
    "    stateDimension = 4\n",
    "    actionDimension = 1\n",
    "    actionSpace = range(2)\n",
    "    transition_sigmas = [ 1e-2, 1e-5, 1e-2, 1e-5 ]\n",
    "    transition_covariance = np.diagflat(transition_sigmas)\n",
    "    logp_mean = (-1/2) * np.log(np.linalg.det(2*np.pi*transition_covariance))\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cartpole_reset(self):\n",
    "        state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        return np.array(state)\n",
    "    \n",
    "    # Extracted from OpenAI environment CartPole-v0\n",
    "    def cartpole_step(self, state, action):\n",
    "\n",
    "        gravity = 9.8\n",
    "        masscart = 1.0\n",
    "        masspole = 0.1\n",
    "        total_mass = (masspole + masscart)\n",
    "        length = 0.5 # actually half the pole's length\n",
    "        polemass_length = (masspole * length)\n",
    "        force_mag = 10.0\n",
    "        tau = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        x_threshold = 2.4\n",
    "\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "\n",
    "        already_done =  x < -x_threshold \\\n",
    "            or x > x_threshold \\\n",
    "            or theta < -theta_threshold_radians \\\n",
    "            or theta > theta_threshold_radians\n",
    "        already_done = bool(already_done)\n",
    "\n",
    "        if already_done:\n",
    "\n",
    "            next_state = state\n",
    "            reward = 0\n",
    "            done = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            force = force_mag if action==1 else -force_mag\n",
    "            costheta = math.cos(theta)\n",
    "            sintheta = math.sin(theta)\n",
    "            temp = (force + polemass_length * theta_dot * theta_dot * sintheta) / total_mass\n",
    "            thetaacc = (gravity * sintheta - costheta* temp) / (length * (4.0/3.0 - masspole * costheta * costheta / total_mass))\n",
    "            xacc  = temp - polemass_length * thetaacc * costheta / total_mass\n",
    "            x  = x + tau * x_dot\n",
    "            x_dot = x_dot + tau * xacc\n",
    "            theta = theta + tau * theta_dot\n",
    "            theta_dot = theta_dot + tau * thetaacc\n",
    "            next_state = np.array([x,x_dot,theta,theta_dot])\n",
    "\n",
    "            reward = 1\n",
    "\n",
    "            done =  x < -x_threshold \\\n",
    "                or x > x_threshold \\\n",
    "                or theta < -theta_threshold_radians \\\n",
    "                or theta > theta_threshold_radians\n",
    "            done = bool(done)\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def noisycartpole_reset(self):\n",
    "        return self.cartpole_reset()\n",
    "\n",
    "    def noisycartpole_step(self, state, action):\n",
    "\n",
    "        next_state_mean, reward, done, info = self.cartpole_step(state, action)   # CartPole Step\n",
    "\n",
    "        if not done:\n",
    "            noise = np.random.randn(self.stateDimension) * self.transition_sigmas        # Adding Noise\n",
    "            logp = multivariate_normal.logpdf( next_state_mean + noise, mean=next_state_mean, cov=self.transition_covariance)\n",
    "        else:\n",
    "            noise = np.zeros(self.stateDimension)\n",
    "            logp = self.logp_mean\n",
    "            \n",
    "        return next_state_mean + noise, reward, done, logp\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.noisycartpole_reset()\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        return self.noisycartpole_step(state, action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = NoisyCartPoleEnvironment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trajectory2tuples(states, actions):\n",
    "\n",
    "    # Dimensions\n",
    "    [sample_count, horizon, state_dimension] = states.shape\n",
    "    [_, _, action_dimension] = actions.shape\n",
    "\n",
    "    # Reshape Inputs and Targets\n",
    "    inputs = np.reshape(states, (sample_count*horizon, state_dimension))\n",
    "    targets = np.reshape(actions, (sample_count*horizon, action_dimension))\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(states, color='red', n=0):\n",
    "\n",
    "    [sample_count, _, _] = states.shape\n",
    "\n",
    "    if n==0:\n",
    "        samples_drawn = range(sample_count)\n",
    "    else:\n",
    "        samples_drawn = np.random.choice(sample_count, n)\n",
    "        \n",
    "    for s in samples_drawn:\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], '-', color=color)\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], 'o', color=color, markersize=2)\n",
    "        plt.plot(states[s, -1, 0], states[s, -1, 2], 'o', color=color, markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_iteration(states, selected=None, n=0):\n",
    "    plot_trajectories(states, color='red', n=n)\n",
    "    if selected is not None:\n",
    "        plot_trajectories(selected, color='green', n=n)\n",
    "    \n",
    "    plt.vlines(0, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(-2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    \n",
    "    plt.hlines(0, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(-0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rollout_trajectories(env, policy, horizon, sample_count=1, init=None):\n",
    "\n",
    "    # States and Actions\n",
    "    states = np.zeros((sample_count, horizon, env.stateDimension))\n",
    "    actions = np.zeros((sample_count, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((sample_count, horizon))\n",
    "    logp = np.zeros(sample_count)\n",
    "    \n",
    "    # Sample Trajectories\n",
    "    for t in range(horizon):\n",
    "        \n",
    "        logp_step_transition = np.zeros((sample_count))\n",
    "\n",
    "        # Initialization\n",
    "        if t == 0:\n",
    "            if init is None:\n",
    "                states[:,t,:] = [ env.reset() for i in range(sample_count) ]\n",
    "            else:\n",
    "                states[:,t,:] = init\n",
    "                \n",
    "        # Transition and Reward\n",
    "        else:\n",
    "            for s in range(sample_count):\n",
    "                states[s, t, :], rewards[s,t-1], _1, logp_step_transition[s] = env.step(states[s, t-1, :], actions[s, t-1, :])\n",
    "        \n",
    "        # Action Selection\n",
    "        actions_unshaped, logp_step_policy = policy.query(states[:, t, :])\n",
    "        actions[:,t,:] = actions_unshaped.reshape(sample_count, env.actionDimension)\n",
    "        \n",
    "        # Log Probability of Sampling\n",
    "        logp += logp_step_transition + logp_step_policy\n",
    "        \n",
    "    for s in range(sample_count):\n",
    "        _, rewards[s, horizon-1], _1, _2 = env.step(states[s, horizon-1, :], actions[s, horizon-1, :])\n",
    "    \n",
    "    return states, actions, rewards, logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SciKitPolicy():\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def query(self, states):\n",
    "        if len(states.shape) == 1:\n",
    "            states = states.reshape(1, -1)\n",
    "        return self.method.predict(states), np.zeros(states.shape[0])\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        self.method.fit(inputs, targets)\n",
    "\n",
    "    def m_step(self, states, actions):\n",
    "\n",
    "        # States/Actions -> Inputs/Targets\n",
    "        inputs, targets = trajectory2tuples(states, actions)\n",
    "\n",
    "        # Train kNN\n",
    "        self.train(inputs, targets.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KnnPolicyDiscrete(SciKitPolicy):\n",
    "    def __init__(self, k, weights='distance'):\n",
    "        self.method = KNeighborsClassifier(n_neighbors=k, weights=weights, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UniformPolicyDiscrete():\n",
    "\n",
    "    def __init__(self, choices):\n",
    "        self.choices = choices\n",
    "\n",
    "    def query(self, states):\n",
    "        return np.random.choice(self.choices, size=states.shape[0]), np.zeros(states.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Sampling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temperatures = [0, 0.5, 1, 5, 10]\n",
    "temperature_count = len(temperatures)\n",
    "\n",
    "def eta(n):\n",
    "    return temperatures[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_noop(env, policy, states_prev, actions_prev, rewards_prev, logp_prev):\n",
    "    return states_prev, actions_prev, rewards_prev, logp_prev, [int(x) for x in range(sample_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_prior(env, policy, states_prev, actions_prev, rewards_prev, logp_prev):\n",
    "    \n",
    "    ancestors = [np.random.randint(sample_count) for x in range(sample_count)]\n",
    "    states, actions, rewards, logp = rollout_trajectories(env, policy, horizon, sample_count)\n",
    "    \n",
    "    return states, actions, rewards, logp, ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smcs(env, policy):\n",
    "    \n",
    "    states = np.zeros((temperature_count, sample_count, horizon, env.stateDimension))\n",
    "    actions = np.zeros((temperature_count, sample_count, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((temperature_count, sample_count, horizon))\n",
    "    logp = np.zeros((temperature_count, sample_count))\n",
    "    ancestors = np.zeros((temperature_count, sample_count))\n",
    "\n",
    "    for n in range(temperature_count):\n",
    "    \n",
    "        if n == 0:\n",
    "            # Initial Trajectories\n",
    "            states[n], actions[n], rewards[n], logp[n] = \\\n",
    "                rollout_trajectories(env, policy, horizon, sample_count)\n",
    "        else:\n",
    "            # Proposing New Trajectories\n",
    "            states[n], actions[n], rewards[n], logp[n], ancestors[n] = \\\n",
    "                kernel_prior(env, policy, states[n-1], actions[n-1], rewards[n-1], logp[n-1] )\n",
    "            \n",
    "            # Weight Calculation\n",
    "            total_rewards = np.sum(rewards[n],axis=1) / horizon\n",
    "            total_rewards_ancestors = np.sum(rewards[n],axis=1) / horizon\n",
    "            \n",
    "            weights = ( total_rewards ** eta(n) / total_rewards_ancestors ** eta(n-1) ) \\\n",
    "                * np.exp(logp[n] - logp[n-1,ancestors[n-1].astype(int)])\n",
    "            \n",
    "            # Resampling\n",
    "            weights = weights / np.sum(weights)\n",
    "            selected = np.random.choice(range(sample_count), size=sample_count, p=weights, replace=True)\n",
    "\n",
    "            states[n] = states[n,selected]\n",
    "            actions[n] = actions[n,selected]\n",
    "            rewards[n] = rewards[n,selected]\n",
    "            logp[n] = logp[n,selected]\n",
    "            ancestors[n] = ancestors[n,selected]\n",
    "        \n",
    "    return states[-1], actions[-1], rewards[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment  \n",
    "horizon = 100\n",
    "\n",
    "# Inference\n",
    "iteration_count = 2\n",
    "sample_count = 10\n",
    "\n",
    "# Policy\n",
    "kNearest = 5\n",
    "policy_sample_count = sample_count\n",
    "selectedPolicy = KnnPolicyDiscrete(kNearest)\n",
    "\n",
    "# Plot\n",
    "rendering_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLEMMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Average Reward : 52.6\n",
      "Iteration 2 - Average Reward : 54.50000000000001\n"
     ]
    }
   ],
   "source": [
    "iteration_rewards = []\n",
    "\n",
    "for i in range(iteration_count):\n",
    "    \n",
    "    if i == 0:\n",
    "        iteration_policy = UniformPolicyDiscrete(env.actionSpace)\n",
    "    else:\n",
    "        iteration_policy = selectedPolicy\n",
    "    \n",
    "    # E-Step\n",
    "    [states, actions, rewards] = smcs(env, iteration_policy)\n",
    "\n",
    "    # M-Step\n",
    "    selectedPolicy.m_step(states, actions)\n",
    "        \n",
    "    # Average Reward\n",
    "    iteration_rewards.append(np.mean(rewards) * horizon)\n",
    "    print( f'Iteration {i+1} - Average Reward : {iteration_rewards[i]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6CWpcAa9JBXOoJekwhn0klQ4g16SCvd/2/mhkh9+y+0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2416918cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iteration_rewards)\n",
    "plt.hlines(np.min([horizon,195]), 0, iteration_count, linestyle='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %lprun -f smcs [states, actions, rewards] = smcs(env, iteration_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f rollout_trajectories [states, actions, rewards] = smcs(env, iteration_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f env.noisycartpole_step [states, actions, rewards] = smcs(env, iteration_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %lprun -f multivariate_normal.logpdf [states, actions, rewards] = smcs(env, iteration_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Probability of Mean in a Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4682446775387277"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multivariate_normal.logpdf(1,mean=1,cov=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4682446775387277"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(1/2) * np.log(2*np.pi*(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covariance = np.array([[1,0],[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8378770664093453"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multivariate_normal.logpdf([1,1],mean=[1,1],cov=covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8378770664093453"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1/2) * np.log(np.linalg.det(2*np.pi*covariance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.44234151813963"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multivariate_normal.logpdf(states[0,0],mean=states[0,0],cov=env.transition_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.442341518139628"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1/2) * np.log(np.linalg.det(2*np.pi*env.transition_covariance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisycartpole_step(state, action):\n",
    "\n",
    "        next_state_mean, reward, done, info = env.cartpole_step(state, action)   # CartPole Step\n",
    "\n",
    "        noise = np.zeros(env.stateDimension)\n",
    "        if not done:\n",
    "            noise = np.random.randn(env.stateDimension) * env.transition_sigmas        # Adding Noise\n",
    "        next_state = next_state_mean + noise\n",
    "\n",
    "        logp = multivariate_normal.logpdf(next_state, mean=next_state_mean, cov=env.transition_covariance)\n",
    "\n",
    "        return next_state, reward, done, logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisycartpole_step_optimized(state, action):\n",
    "\n",
    "        next_state_mean, reward, done, info = env.cartpole_step(state, action)   # CartPole Step\n",
    "\n",
    "        if not done:\n",
    "            noise = np.random.randn(env.stateDimension) * env.transition_sigmas        # Adding Noise\n",
    "            logp = multivariate_normal.logpdf( next_state_mean + noise, mean=next_state_mean, cov=env.transition_covariance)\n",
    "        else:\n",
    "            noise = np.zeros(env.stateDimension)\n",
    "            logp = env.logp_mean\n",
    "        \n",
    "        return next_state_mean + noise, reward, done, logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = [noisycartpole_step(states[0,-1], actions[0,-1]) for x in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time _ = [noisycartpole_step_optimized(states[0,-1], actions[0,-1]) for x in range(10000)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
