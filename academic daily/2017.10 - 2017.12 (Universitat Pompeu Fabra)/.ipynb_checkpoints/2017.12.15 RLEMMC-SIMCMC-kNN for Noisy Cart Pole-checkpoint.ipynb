{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Cart Pole\n",
    "\n",
    "This is the noisy version of the CartPole-v0 environment of OpenAI.  \n",
    "https://gym.openai.com/envs/CartPole-v0  \n",
    "https://github.com/openai/gym/wiki/CartPole-v0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoisyCartPoleEnvironment:\n",
    "    \n",
    "    stateDimension = 4\n",
    "    actionDimension = 1\n",
    "    actionSpace = range(2)\n",
    "    transitionSigmas = [ 1e-2, 1e-5, 1e-2, 1e-5 ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cartpole_reset(self):\n",
    "        state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        return np.array(state)\n",
    "    \n",
    "    # Extracted from OpenAI environment CartPole-v0\n",
    "    def cartpole_step(self, state, action):\n",
    "\n",
    "        gravity = 9.8\n",
    "        masscart = 1.0\n",
    "        masspole = 0.1\n",
    "        total_mass = (masspole + masscart)\n",
    "        length = 0.5 # actually half the pole's length\n",
    "        polemass_length = (masspole * length)\n",
    "        force_mag = 10.0\n",
    "        tau = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        x_threshold = 2.4\n",
    "\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "\n",
    "        already_done =  x < -x_threshold \\\n",
    "            or x > x_threshold \\\n",
    "            or theta < -theta_threshold_radians \\\n",
    "            or theta > theta_threshold_radians\n",
    "        already_done = bool(already_done)\n",
    "\n",
    "        if already_done:\n",
    "\n",
    "            next_state = state\n",
    "            reward = 0\n",
    "            done = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            force = force_mag if action==1 else -force_mag\n",
    "            costheta = math.cos(theta)\n",
    "            sintheta = math.sin(theta)\n",
    "            temp = (force + polemass_length * theta_dot * theta_dot * sintheta) / total_mass\n",
    "            thetaacc = (gravity * sintheta - costheta* temp) / (length * (4.0/3.0 - masspole * costheta * costheta / total_mass))\n",
    "            xacc  = temp - polemass_length * thetaacc * costheta / total_mass\n",
    "            x  = x + tau * x_dot\n",
    "            x_dot = x_dot + tau * xacc\n",
    "            theta = theta + tau * theta_dot\n",
    "            theta_dot = theta_dot + tau * thetaacc\n",
    "            next_state = np.array([x,x_dot,theta,theta_dot])\n",
    "\n",
    "            reward = 1\n",
    "\n",
    "            done =  x < -x_threshold \\\n",
    "                or x > x_threshold \\\n",
    "                or theta < -theta_threshold_radians \\\n",
    "                or theta > theta_threshold_radians\n",
    "            done = bool(done)\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def noisycartpole_reset(self):\n",
    "        return self.cartpole_reset()\n",
    "\n",
    "    def noisycartpole_step(self, state, action):\n",
    "\n",
    "        next_state_mean, reward, done, info = self.cartpole_step(state, action)   # CartPole Step\n",
    "\n",
    "        noise = np.zeros(self.stateDimension)\n",
    "        if not done:\n",
    "            noise = np.random.randn(self.stateDimension) * self.transitionSigmas        # Adding Noise\n",
    "        next_state = next_state_mean + noise\n",
    "\n",
    "        logp = multivariate_normal.logpdf(next_state, mean=next_state_mean, cov=np.diagflat(self.transitionSigmas))\n",
    "\n",
    "        return next_state, reward, done, logp\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.noisycartpole_reset()\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        return self.noisycartpole_step(state, action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = NoisyCartPoleEnvironment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trajectory2tuples(states, actions):\n",
    "\n",
    "    # Dimensions\n",
    "    [sample_count, horizon, state_dimension] = states.shape\n",
    "    [_, _, action_dimension] = actions.shape\n",
    "\n",
    "    # Reshape Inputs and Targets\n",
    "    inputs = np.reshape(states, (sample_count*horizon, state_dimension))\n",
    "    targets = np.reshape(actions, (sample_count*horizon, action_dimension))\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(states, color='red', n=0):\n",
    "\n",
    "    [sample_count, _, _] = states.shape\n",
    "\n",
    "    if n==0:\n",
    "        samples_drawn = range(sample_count)\n",
    "    else:\n",
    "        samples_drawn = np.random.choice(sample_count, n)\n",
    "        \n",
    "    for s in samples_drawn:\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], '-', color=color)\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], 'o', color=color, markersize=2)\n",
    "        plt.plot(states[s, -1, 0], states[s, -1, 2], 'o', color=color, markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_iteration(states, selected=None, n=0):\n",
    "    plot_trajectories(states, color='red', n=n)\n",
    "    if selected is not None:\n",
    "        plot_trajectories(selected, color='green', n=n)\n",
    "    \n",
    "    plt.vlines(0, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(-2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    \n",
    "    plt.hlines(0, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(-0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rollout_trajectories(env, policy, horizon, sample_count=1, init=None):\n",
    "\n",
    "    # States and Actions\n",
    "    states = np.zeros((sample_count, horizon, env.stateDimension))\n",
    "    actions = np.zeros((sample_count, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((sample_count, horizon))\n",
    "    logp = np.zeros(sample_count)\n",
    "    \n",
    "    # Sample Trajectories\n",
    "    for t in range(horizon):\n",
    "        \n",
    "        logp_step_transition = np.zeros((sample_count))\n",
    "\n",
    "        # Initialization\n",
    "        if t == 0:\n",
    "            if init is None:\n",
    "                states[:,t,:] = [ env.reset() for i in range(sample_count) ]\n",
    "            else:\n",
    "                states[:,t,:] = init\n",
    "                \n",
    "        # Transition and Reward\n",
    "        else:\n",
    "            for s in range(sample_count):\n",
    "                states[s, t, :], rewards[s,t-1], _1, logp_step_transition[s] = env.step(states[s, t-1, :], actions[s, t-1, :])\n",
    "        \n",
    "        # Action Selection\n",
    "        actions_unshaped, logp_step_policy = policy.query(states[:, t, :])\n",
    "        actions[:,t,:] = actions_unshaped.reshape(sample_count, env.actionDimension)\n",
    "        \n",
    "        # Log Probability of Sampling\n",
    "        logp += logp_step_transition + logp_step_policy\n",
    "        \n",
    "    for s in range(sample_count):\n",
    "        _, rewards[s, horizon-1], _1, _2 = env.step(states[s, horizon-1, :], actions[s, horizon-1, :])\n",
    "    \n",
    "    return states, actions, rewards, logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SciKitPolicy():\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def query(self, states):\n",
    "        if len(states.shape) == 1:\n",
    "            states = states.reshape(1, -1)\n",
    "        return self.method.predict(states), np.zeros(states.shape[0])\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        self.method.fit(inputs, targets)\n",
    "\n",
    "    def m_step(self, states, actions):\n",
    "\n",
    "        # States/Actions -> Inputs/Targets\n",
    "        inputs, targets = trajectory2tuples(states, actions)\n",
    "\n",
    "        # Train kNN\n",
    "        self.train(inputs, targets.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KnnPolicyDiscrete(SciKitPolicy):\n",
    "    def __init__(self, k, weights='distance'):\n",
    "        self.method = KNeighborsClassifier(n_neighbors=k, weights=weights, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UniformPolicyDiscrete():\n",
    "\n",
    "    def __init__(self, choices):\n",
    "        self.choices = choices\n",
    "\n",
    "    def query(self, states):\n",
    "        return np.random.choice(self.choices, size=states.shape[0]), np.zeros(states.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Sampling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temperatures = [0, 0.5, 1, 5, 10]\n",
    "temperature_count = len(temperatures)\n",
    "\n",
    "def eta(n):\n",
    "    return temperatures[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_noop(env, policy, states_prev, actions_prev, rewards_prev, logp_prev):\n",
    "    return states_prev, actions_prev, rewards_prev, logp_prev, [int(x) for x in range(sample_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_prior(env, policy, states_prev, actions_prev, rewards_prev, logp_prev):\n",
    "    \n",
    "    ancestors = [np.random.randint(sample_count) for x in range(sample_count)]\n",
    "    states, actions, rewards, logp = rollout_trajectories(env, policy, horizon, sample_count)\n",
    "    \n",
    "    return states, actions, rewards, logp, ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smcs(env, policy):\n",
    "    \n",
    "    states = np.zeros((temperature_count, sample_count, horizon, env.stateDimension))\n",
    "    actions = np.zeros((temperature_count, sample_count, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((temperature_count, sample_count, horizon))\n",
    "    logp = np.zeros((temperature_count, sample_count))\n",
    "    ancestors = np.zeros((temperature_count, sample_count))\n",
    "\n",
    "    for n in range(temperature_count):\n",
    "    \n",
    "        if n == 0:\n",
    "            # Initial Trajectories\n",
    "            states[n], actions[n], rewards[n], logp[n] = \\\n",
    "                rollout_trajectories(env, policy, horizon, sample_count)\n",
    "        else:\n",
    "            # Proposing New Trajectories\n",
    "            states[n], actions[n], rewards[n], logp[n], ancestors[n] = \\\n",
    "                kernel_prior(env, policy, states[n-1], actions[n-1], rewards[n-1], logp[n-1] )\n",
    "            \n",
    "            # Weight Calculation\n",
    "            total_rewards = np.sum(rewards[n],axis=1) / horizon\n",
    "            total_rewards_ancestors = np.sum(rewards[n],axis=1) / horizon\n",
    "            \n",
    "            weights = ( total_rewards ** eta(n) / total_rewards_ancestors ** eta(n-1) ) \\\n",
    "                * np.exp(logp[n] - logp[n-1,ancestors[n-1].astype(int)])\n",
    "            \n",
    "            # Resampling\n",
    "            weights = weights / np.sum(weights)\n",
    "            selected = np.random.choice(range(sample_count), size=sample_count, p=weights, replace=True)\n",
    "\n",
    "            states[n] = states[n,selected]\n",
    "            actions[n] = actions[n,selected]\n",
    "            rewards[n] = rewards[n,selected]\n",
    "            logp[n] = logp[n,selected]\n",
    "            ancestors[n] = ancestors[n,selected]\n",
    "        \n",
    "    return states[-1], actions[-1], rewards[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment  \n",
    "horizon = 250\n",
    "\n",
    "# Inference\n",
    "iteration_count = 50\n",
    "sample_count = 100\n",
    "\n",
    "# Policy\n",
    "kNearest = 5\n",
    "policy_sample_count = sample_count\n",
    "selectedPolicy = KnnPolicyDiscrete(kNearest)\n",
    "\n",
    "# Plot\n",
    "rendering_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLEMMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Average Reward : 44.46\n",
      "Iteration 2 - Average Reward : 127.07\n",
      "Iteration 3 - Average Reward : 145.65\n",
      "Iteration 4 - Average Reward : 188.76000000000002\n",
      "Iteration 5 - Average Reward : 203.08\n",
      "Iteration 6 - Average Reward : 215.57000000000002\n",
      "Iteration 7 - Average Reward : 211.38000000000002\n",
      "Iteration 8 - Average Reward : 221.58\n",
      "Iteration 9 - Average Reward : 227.54\n",
      "Iteration 10 - Average Reward : 224.89999999999998\n",
      "Iteration 11 - Average Reward : 232.45\n",
      "Iteration 12 - Average Reward : 232.81\n",
      "Iteration 13 - Average Reward : 232.71\n",
      "Iteration 14 - Average Reward : 235.25\n",
      "Iteration 15 - Average Reward : 230.08999999999997\n",
      "Iteration 16 - Average Reward : 231.04\n",
      "Iteration 17 - Average Reward : 225.06\n",
      "Iteration 18 - Average Reward : 233.61\n",
      "Iteration 19 - Average Reward : 229.71\n",
      "Iteration 20 - Average Reward : 240.27\n",
      "Iteration 21 - Average Reward : 228.02\n",
      "Iteration 22 - Average Reward : 229.66\n",
      "Iteration 23 - Average Reward : 233.86\n",
      "Iteration 24 - Average Reward : 230.43\n",
      "Iteration 25 - Average Reward : 230.97\n",
      "Iteration 26 - Average Reward : 231.92\n",
      "Iteration 27 - Average Reward : 220.17000000000002\n",
      "Iteration 28 - Average Reward : 216.35999999999999\n",
      "Iteration 29 - Average Reward : 219.94\n",
      "Iteration 30 - Average Reward : 224.02\n",
      "Iteration 31 - Average Reward : 230.36\n",
      "Iteration 32 - Average Reward : 216.32000000000002\n",
      "Iteration 33 - Average Reward : 224.54\n",
      "Iteration 34 - Average Reward : 223.27\n",
      "Iteration 35 - Average Reward : 230.51\n",
      "Iteration 36 - Average Reward : 224.42999999999998\n",
      "Iteration 37 - Average Reward : 225.66\n",
      "Iteration 38 - Average Reward : 233.84\n",
      "Iteration 39 - Average Reward : 231.0\n",
      "Iteration 40 - Average Reward : 226.36\n",
      "Iteration 41 - Average Reward : 215.45\n",
      "Iteration 42 - Average Reward : 220.82\n",
      "Iteration 43 - Average Reward : 221.45000000000002\n",
      "Iteration 44 - Average Reward : 225.77\n",
      "Iteration 45 - Average Reward : 226.14000000000001\n",
      "Iteration 46 - Average Reward : 220.34\n",
      "Iteration 47 - Average Reward : 215.04999999999998\n",
      "Iteration 48 - Average Reward : 227.14000000000001\n",
      "Iteration 49 - Average Reward : 218.73000000000002\n",
      "Iteration 50 - Average Reward : 220.83\n"
     ]
    }
   ],
   "source": [
    "iteration_rewards = []\n",
    "\n",
    "for i in range(iteration_count):\n",
    "    \n",
    "    if i == 0:\n",
    "        iteration_policy = UniformPolicyDiscrete(env.actionSpace)\n",
    "    else:\n",
    "        iteration_policy = selectedPolicy\n",
    "    \n",
    "    # E-Step\n",
    "    [states, actions, rewards] = smcs(env, iteration_policy)\n",
    "\n",
    "    # M-Step\n",
    "    selectedPolicy.m_step(states, actions)\n",
    "        \n",
    "    # Average Reward\n",
    "    iteration_rewards.append(np.mean(rewards) * horizon)\n",
    "    print( f'Iteration {i+1} - Average Reward : {iteration_rewards[i]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23065393390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iteration_rewards)\n",
    "plt.hlines(np.min([horizon,195]), 0, iteration_count, linestyle='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "def render_state(env, t):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Timestep : %s\" %(env.spec.id, t))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if rendering_enabled:\n",
    "    env = gym.make('CartPole-v0')\n",
    "\n",
    "    state = env.reset()\n",
    "    for t in range(horizon):\n",
    "        render_state(env,t)\n",
    "        state,_,done,_ = env.step(int(iteration_policy.query(state)))\n",
    "        if done:\n",
    "            break        \n",
    "    render_state(env,t)\n",
    "\n",
    "    env.render(close=True)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
