{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Cart Pole\n",
    "\n",
    "This is the noisy version of the CartPole-v0 environment of OpenAI.  \n",
    "https://gym.openai.com/envs/CartPole-v0  \n",
    "https://github.com/openai/gym/wiki/CartPole-v0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoisyCartPoleEnvironment:\n",
    "    \n",
    "    stateDimension = 4\n",
    "    actionDimension = 1\n",
    "    actionSpace = range(2)\n",
    "    transitionSigmas = [ 1e-2, 1e-5, 1e-2, 1e-5 ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cartpole_reset(self):\n",
    "        state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        return np.array(state)\n",
    "    \n",
    "    # Extracted from OpenAI environment CartPole-v0\n",
    "    def cartpole_step(self, state, action):\n",
    "\n",
    "        gravity = 9.8\n",
    "        masscart = 1.0\n",
    "        masspole = 0.1\n",
    "        total_mass = (masspole + masscart)\n",
    "        length = 0.5 # actually half the pole's length\n",
    "        polemass_length = (masspole * length)\n",
    "        force_mag = 10.0\n",
    "        tau = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        x_threshold = 2.4\n",
    "\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "\n",
    "        already_done =  x < -x_threshold \\\n",
    "            or x > x_threshold \\\n",
    "            or theta < -theta_threshold_radians \\\n",
    "            or theta > theta_threshold_radians\n",
    "        already_done = bool(already_done)\n",
    "\n",
    "        if already_done:\n",
    "\n",
    "            next_state = state\n",
    "            reward = 0\n",
    "            done = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            force = force_mag if action==1 else -force_mag\n",
    "            costheta = math.cos(theta)\n",
    "            sintheta = math.sin(theta)\n",
    "            temp = (force + polemass_length * theta_dot * theta_dot * sintheta) / total_mass\n",
    "            thetaacc = (gravity * sintheta - costheta* temp) / (length * (4.0/3.0 - masspole * costheta * costheta / total_mass))\n",
    "            xacc  = temp - polemass_length * thetaacc * costheta / total_mass\n",
    "            x  = x + tau * x_dot\n",
    "            x_dot = x_dot + tau * xacc\n",
    "            theta = theta + tau * theta_dot\n",
    "            theta_dot = theta_dot + tau * thetaacc\n",
    "            next_state = np.array([x,x_dot,theta,theta_dot])\n",
    "\n",
    "            reward = 1\n",
    "\n",
    "            done =  x < -x_threshold \\\n",
    "                or x > x_threshold \\\n",
    "                or theta < -theta_threshold_radians \\\n",
    "                or theta > theta_threshold_radians\n",
    "            done = bool(done)\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def noisycartpole_reset(self):\n",
    "        return self.cartpole_reset()\n",
    "\n",
    "    def noisycartpole_step(self, state, action):\n",
    "\n",
    "        next_state_mean, reward, done, info = self.cartpole_step(state, action)   # CartPole Step\n",
    "\n",
    "        noise = np.zeros(self.stateDimension)\n",
    "        if not done:\n",
    "            noise = np.random.randn(self.stateDimension) * self.transitionSigmas        # Adding Noise\n",
    "        next_state = next_state_mean + noise\n",
    "\n",
    "        logp = multivariate_normal.logpdf(next_state, mean=next_state_mean, cov=np.diagflat(self.transitionSigmas))\n",
    "\n",
    "        return next_state, reward, done, logp\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.noisycartpole_reset()\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        return self.noisycartpole_step(state, action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NoisyCartPoleEnvironment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SciKitPolicy():\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def query(self, states):\n",
    "        if len(states.shape) == 1:\n",
    "            states = states.reshape(1, -1)\n",
    "        return self.method.predict(states)\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        self.method.fit(inputs, targets)\n",
    "\n",
    "    def m_step(self, states, actions):\n",
    "\n",
    "        # States/Actions -> Inputs/Targets\n",
    "        inputs, targets = trajectory2tuples(states, actions)\n",
    "\n",
    "        # Train kNN\n",
    "        self.train(inputs, targets.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UniformPolicyDiscrete():\n",
    "\n",
    "    def __init__(self, choices):\n",
    "        self.choices = choices\n",
    "\n",
    "    def query(self, states):\n",
    "        return np.random.choice(self.choices, size=states.shape[0]), np.zeros(states.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_trajectories(env, policy, horizon, sample_count=1, init=None):\n",
    "\n",
    "    # States and Actions\n",
    "    states = np.zeros((sample_count, horizon, env.stateDimension))\n",
    "    actions = np.zeros((sample_count, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((sample_count, horizon))\n",
    "    logp = np.zeros(sample_count)\n",
    "    \n",
    "    # Sample Trajectories\n",
    "    for t in range(horizon):\n",
    "        \n",
    "        logp_step_transition = np.zeros((sample_count))\n",
    "\n",
    "        # Initialization\n",
    "        if t == 0:\n",
    "            if init is None:\n",
    "                states[:,t,:] = [ env.reset() for i in range(sample_count) ]\n",
    "            else:\n",
    "                states[:,t,:] = init\n",
    "                \n",
    "        # Transition and Reward\n",
    "        else:\n",
    "            for s in range(sample_count):\n",
    "                states[s, t, :], rewards[s,t-1], _1, logp_step_transition[s] = env.step(states[s, t-1, :], actions[s, t-1, :])\n",
    "        \n",
    "        # Action Selection\n",
    "        actions_unshaped, logp_step_policy = policy.query(states[:, t, :])\n",
    "        actions[:,t,:] = actions_unshaped.reshape(sample_count, env.actionDimension)\n",
    "        \n",
    "        # Log Probability of Sampling\n",
    "        logp += logp_step_transition + logp_step_policy\n",
    "        \n",
    "    for s in range(sample_count):\n",
    "        _, rewards[s, horizon-1], _1, _2 = env.step(states[s, horizon-1, :], actions[s, horizon-1, :])\n",
    "\n",
    "    return states, actions, rewards, logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(states, color='red', n=0):\n",
    "\n",
    "    [sample_count, _, _] = states.shape\n",
    "\n",
    "    if n==0:\n",
    "        samples_drawn = range(sample_count)\n",
    "    else:\n",
    "        samples_drawn = np.random.choice(sample_count, n)\n",
    "        \n",
    "    for s in samples_drawn:\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], '-', color=color)\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], 'o', color=color, markersize=2)\n",
    "        plt.plot(states[s, -1, 0], states[s, -1, 2], 'o', color=color, markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_iteration(states, rewards, selected=None, n=0, iteration=None, temperature=None):\n",
    "    \n",
    "    plot_trajectories(states, color='red', n=n)\n",
    "    if selected is not None:\n",
    "        plot_trajectories(selected, color='green', n=n)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.vlines(0, -0.25, 0.25, linestyle='dotted')\n",
    "    plt.hlines(0, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(-0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    \n",
    "    label = ''\n",
    "    if iteration is not None:\n",
    "        label += 'Iteration ' + str(iteration+1) + ' - '\n",
    "    if temperature is not None:\n",
    "        label += 'Temperature ' + str(temperature+1) + ' - '\n",
    "    label +=  'Average Reward : ' + str(np.sum(rewards) / states.shape[0])\n",
    "    plt.xlabel(label)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment  \n",
    "horizon = 100\n",
    "\n",
    "# Inference\n",
    "sample_count = 10\n",
    "\n",
    "# Policy\n",
    "policy = UniformPolicyDiscrete(env.actionSpace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Monte Carlo Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temperatures = [0, 0.5, 1]\n",
    "temperature_count = len(temperatures)\n",
    "\n",
    "def eta(n):\n",
    "    return temperatures[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.zeros((temperature_count, sample_count, horizon, env.stateDimension))\n",
    "actions = np.zeros((temperature_count, sample_count, horizon, env.actionDimension))\n",
    "rewards = np.zeros((temperature_count, sample_count, horizon))\n",
    "logp = np.zeros((temperature_count, sample_count))\n",
    "ancestors = np.zeros((temperature_count, sample_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_noop(states_prev, actions_prev, rewards_prev, logp_prev):\n",
    "    return states_prev, actions_prev, rewards_prev, logp_prev, [int(x) for x in range(sample_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bridge Density : #1/4  - eta(n) =  0\n",
      "Bridge Density : #2/4  - eta(n) =  0.5\n",
      "[7 5 4 7 1 6 8 7 8 8]\n",
      "Bridge Density : #3/4  - eta(n) =  1\n",
      "[4 8 6 4 5 8 5 1 2 4]\n"
     ]
    }
   ],
   "source": [
    "for n in range(temperature_count):\n",
    "    \n",
    "    if n == 0:\n",
    "        states[n,:,:,:], actions[n,:,:,:], rewards[n,:,:], logp[n,:] = \\\n",
    "            rollout_trajectories(env, policy, horizon, sample_count)\n",
    "    else:\n",
    "        states[n,:,:,:], actions[n,:,:,:], rewards[n,:,:], logp[n,:], ancestors[n,:] = \\\n",
    "            kernel_noop(states[n-1,:,:,:], actions[n-1,:,:,:], rewards[n-1,:,:], logp[n-1,:] )\n",
    "    \n",
    "        total_rewards = np.sum(rewards[n,:,:],axis=1) / horizon\n",
    "        total_rewards_ancestors = np.sum(rewards[n,:,:],axis=1) / horizon\n",
    "    \n",
    "        weights = ( total_rewards ** eta(n) / total_rewards_ancestors ** eta(n-1) ) \\\n",
    "            * np.exp(logp[n,:] - logp[n-1,ancestors[n-1,:].astype(int)])\n",
    "    \n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        selected = np.random.choice(range(sample_count), size=sample_count, p=weights, replace=True)\n",
    "        \n",
    "        states[n,:,:,:] = states[n,selected,:,:]\n",
    "        actions[n,:,:,:] = actions[n,selected,:]\n",
    "        rewards[n,:,:] = rewards[n,selected,:]\n",
    "        logp[n,:] = logp[n,selected]\n",
    "        ancestors[n,:] = ancestors[n,selected]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
