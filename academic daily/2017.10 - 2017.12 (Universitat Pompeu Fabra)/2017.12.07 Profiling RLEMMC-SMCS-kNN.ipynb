{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Cart Pole\n",
    "\n",
    "This is the noisy version of the CartPole-v0 environment of OpenAI.  \n",
    "https://gym.openai.com/envs/CartPole-v0  \n",
    "https://github.com/openai/gym/wiki/CartPole-v0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoisyCartPoleEnvironment:\n",
    "    \n",
    "    stateDimension = 4\n",
    "    actionDimension = 1\n",
    "    actionSpace = range(2)\n",
    "    transition_sigmas = [ 1e-2, 1e-5, 1e-2, 1e-5 ]\n",
    "    transition_covariance = np.diagflat(transition_sigmas)\n",
    "    logp_mean = (-1/2) * np.log(np.linalg.det(2*np.pi*transition_covariance))\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cartpole_reset(self):\n",
    "        state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        return np.array(state)\n",
    "    \n",
    "    # Extracted from OpenAI environment CartPole-v0\n",
    "    def cartpole_step(self, state, action):\n",
    "\n",
    "        gravity = 9.8\n",
    "        masscart = 1.0\n",
    "        masspole = 0.1\n",
    "        total_mass = (masspole + masscart)\n",
    "        length = 0.5 # actually half the pole's length\n",
    "        polemass_length = (masspole * length)\n",
    "        force_mag = 10.0\n",
    "        tau = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        x_threshold = 2.4\n",
    "\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "\n",
    "        already_done =  x < -x_threshold \\\n",
    "            or x > x_threshold \\\n",
    "            or theta < -theta_threshold_radians \\\n",
    "            or theta > theta_threshold_radians\n",
    "        already_done = bool(already_done)\n",
    "\n",
    "        if already_done:\n",
    "\n",
    "            next_state = state\n",
    "            reward = 0\n",
    "            done = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            force = force_mag if action==1 else -force_mag\n",
    "            costheta = math.cos(theta)\n",
    "            sintheta = math.sin(theta)\n",
    "            temp = (force + polemass_length * theta_dot * theta_dot * sintheta) / total_mass\n",
    "            thetaacc = (gravity * sintheta - costheta* temp) / (length * (4.0/3.0 - masspole * costheta * costheta / total_mass))\n",
    "            xacc  = temp - polemass_length * thetaacc * costheta / total_mass\n",
    "            x  = x + tau * x_dot\n",
    "            x_dot = x_dot + tau * xacc\n",
    "            theta = theta + tau * theta_dot\n",
    "            theta_dot = theta_dot + tau * thetaacc\n",
    "            next_state = np.array([x,x_dot,theta,theta_dot])\n",
    "\n",
    "            reward = 1\n",
    "\n",
    "            done =  x < -x_threshold \\\n",
    "                or x > x_threshold \\\n",
    "                or theta < -theta_threshold_radians \\\n",
    "                or theta > theta_threshold_radians\n",
    "            done = bool(done)\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def noisycartpole_reset(self):\n",
    "        return self.cartpole_reset()\n",
    "\n",
    "    def noisycartpole_step(self, state, action):\n",
    "\n",
    "        next_state_mean, reward, done, info = self.cartpole_step(state, action)   # CartPole Step\n",
    "\n",
    "        if not done:\n",
    "            noise = np.random.randn(self.stateDimension) * self.transition_sigmas        # Adding Noise\n",
    "            logp = multivariate_normal.logpdf( next_state_mean + noise, mean=next_state_mean, cov=self.transition_covariance)\n",
    "        else:\n",
    "            noise = np.zeros(self.stateDimension)\n",
    "            logp = self.logp_mean\n",
    "            \n",
    "        return next_state_mean + noise, reward, done, logp\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.noisycartpole_reset()\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        return self.noisycartpole_step(state, action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = NoisyCartPoleEnvironment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trajectory2tuples(states, actions):\n",
    "\n",
    "    # Dimensions\n",
    "    [sample_count, horizon, state_dimension] = states.shape\n",
    "    [_, _, action_dimension] = actions.shape\n",
    "\n",
    "    # Reshape Inputs and Targets\n",
    "    inputs = np.reshape(states, (sample_count*horizon, state_dimension))\n",
    "    targets = np.reshape(actions, (sample_count*horizon, action_dimension))\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(states, color='red', n=0):\n",
    "\n",
    "    [sample_count, _, _] = states.shape\n",
    "\n",
    "    if n==0:\n",
    "        samples_drawn = range(sample_count)\n",
    "    else:\n",
    "        samples_drawn = np.random.choice(sample_count, n)\n",
    "        \n",
    "    for s in samples_drawn:\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], '-', color=color)\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], 'o', color=color, markersize=2)\n",
    "        plt.plot(states[s, -1, 0], states[s, -1, 2], 'o', color=color, markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_iteration(states, selected=None, n=0):\n",
    "    plot_trajectories(states, color='red', n=n)\n",
    "    if selected is not None:\n",
    "        plot_trajectories(selected, color='green', n=n)\n",
    "    \n",
    "    plt.vlines(0, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(-2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    \n",
    "    plt.hlines(0, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(-0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rollout_trajectories(env, policy, horizon, sample_count=1, init=None):\n",
    "\n",
    "    # States and Actions\n",
    "    states = np.zeros((sample_count, horizon, env.stateDimension))\n",
    "    actions = np.zeros((sample_count, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((sample_count, horizon))\n",
    "    logp = np.zeros(sample_count)\n",
    "    \n",
    "    # Sample Trajectories\n",
    "    for t in range(horizon):\n",
    "        \n",
    "        logp_step_transition = np.zeros((sample_count))\n",
    "\n",
    "        # Initialization\n",
    "        if t == 0:\n",
    "            if init is None:\n",
    "                states[:,t,:] = [ env.reset() for i in range(sample_count) ]\n",
    "            else:\n",
    "                states[:,t,:] = init\n",
    "                \n",
    "        # Transition and Reward\n",
    "        else:\n",
    "            for s in range(sample_count):\n",
    "                states[s, t, :], rewards[s,t-1], _1, logp_step_transition[s] = env.step(states[s, t-1, :], actions[s, t-1, :])\n",
    "        \n",
    "        # Action Selection\n",
    "        actions_unshaped, logp_step_policy = policy.query(states[:, t, :])\n",
    "        actions[:,t,:] = actions_unshaped.reshape(sample_count, env.actionDimension)\n",
    "        \n",
    "        # Log Probability of Sampling\n",
    "        logp += logp_step_transition + logp_step_policy\n",
    "        \n",
    "    for s in range(sample_count):\n",
    "        _, rewards[s, horizon-1], _1, _2 = env.step(states[s, horizon-1, :], actions[s, horizon-1, :])\n",
    "    \n",
    "    return states, actions, rewards, logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SciKitPolicy():\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def query(self, states):\n",
    "        if len(states.shape) == 1:\n",
    "            states = states.reshape(1, -1)\n",
    "        return self.method.predict(states), np.zeros(states.shape[0])\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        self.method.fit(inputs, targets)\n",
    "\n",
    "    def m_step(self, states, actions):\n",
    "\n",
    "        # States/Actions -> Inputs/Targets\n",
    "        inputs, targets = trajectory2tuples(states, actions)\n",
    "\n",
    "        # Train kNN\n",
    "        self.train(inputs, targets.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KnnPolicyDiscrete(SciKitPolicy):\n",
    "    def __init__(self, k, weights='distance'):\n",
    "        self.method = KNeighborsClassifier(n_neighbors=k, weights=weights, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UniformPolicyDiscrete():\n",
    "\n",
    "    def __init__(self, choices):\n",
    "        self.choices = choices\n",
    "\n",
    "    def query(self, states):\n",
    "        return np.random.choice(self.choices, size=states.shape[0]), np.zeros(states.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Sampling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temperatures = [0, 0.5, 1, 5, 10]\n",
    "temperature_count = len(temperatures)\n",
    "\n",
    "def eta(n):\n",
    "    return temperatures[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_noop(env, policy, states_prev, actions_prev, rewards_prev, logp_prev):\n",
    "    return states_prev, actions_prev, rewards_prev, logp_prev, [int(x) for x in range(sample_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_prior(env, policy, states_prev, actions_prev, rewards_prev, logp_prev):\n",
    "    \n",
    "    ancestors = [np.random.randint(sample_count) for x in range(sample_count)]\n",
    "    states, actions, rewards, logp = rollout_trajectories(env, policy, horizon, sample_count)\n",
    "    \n",
    "    return states, actions, rewards, logp, ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smcs(env, policy):\n",
    "    \n",
    "    states = np.zeros((temperature_count, sample_count, horizon, env.stateDimension))\n",
    "    actions = np.zeros((temperature_count, sample_count, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((temperature_count, sample_count, horizon))\n",
    "    logp = np.zeros((temperature_count, sample_count))\n",
    "    ancestors = np.zeros((temperature_count, sample_count))\n",
    "\n",
    "    for n in range(temperature_count):\n",
    "    \n",
    "        if n == 0:\n",
    "            # Initial Trajectories\n",
    "            states[n], actions[n], rewards[n], logp[n] = \\\n",
    "                rollout_trajectories(env, policy, horizon, sample_count)\n",
    "        else:\n",
    "            # Proposing New Trajectories\n",
    "            states[n], actions[n], rewards[n], logp[n], ancestors[n] = \\\n",
    "                kernel_prior(env, policy, states[n-1], actions[n-1], rewards[n-1], logp[n-1] )\n",
    "            \n",
    "            # Weight Calculation\n",
    "            total_rewards = np.sum(rewards[n],axis=1) / horizon\n",
    "            total_rewards_ancestors = np.sum(rewards[n],axis=1) / horizon\n",
    "            \n",
    "            weights = ( total_rewards ** eta(n) / total_rewards_ancestors ** eta(n-1) ) \\\n",
    "                * np.exp(logp[n] - logp[n-1,ancestors[n-1].astype(int)])\n",
    "            \n",
    "            # Resampling\n",
    "            weights = weights / np.sum(weights)\n",
    "            selected = np.random.choice(range(sample_count), size=sample_count, p=weights, replace=True)\n",
    "\n",
    "            states[n] = states[n,selected]\n",
    "            actions[n] = actions[n,selected]\n",
    "            rewards[n] = rewards[n,selected]\n",
    "            logp[n] = logp[n,selected]\n",
    "            ancestors[n] = ancestors[n,selected]\n",
    "        \n",
    "    return states[-1], actions[-1], rewards[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment  \n",
    "horizon = 100\n",
    "\n",
    "# Inference\n",
    "iteration_count = 2\n",
    "sample_count = 10\n",
    "\n",
    "# Policy\n",
    "kNearest = 5\n",
    "policy_sample_count = sample_count\n",
    "selectedPolicy = KnnPolicyDiscrete(kNearest)\n",
    "\n",
    "# Plot\n",
    "rendering_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLEMMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Average Reward : 52.2\n",
      "Iteration 2 - Average Reward : 64.3\n"
     ]
    }
   ],
   "source": [
    "iteration_rewards = []\n",
    "\n",
    "for i in range(iteration_count):\n",
    "    \n",
    "    if i == 0:\n",
    "        iteration_policy = UniformPolicyDiscrete(env.actionSpace)\n",
    "    else:\n",
    "        iteration_policy = selectedPolicy\n",
    "    \n",
    "    # E-Step\n",
    "    [states, actions, rewards] = smcs(env, iteration_policy)\n",
    "\n",
    "    # M-Step\n",
    "    selectedPolicy.m_step(states, actions)\n",
    "        \n",
    "    # Average Reward\n",
    "    iteration_rewards.append(np.mean(rewards) * horizon)\n",
    "    print( f'Iteration {i+1} - Average Reward : {iteration_rewards[i]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e7b0e5df28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iteration_rewards)\n",
    "plt.hlines(np.min([horizon,195]), 0, iteration_count, linestyle='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %lprun -f smcs [states, actions, rewards] = smcs(env, iteration_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f rollout_trajectories [states, actions, rewards] = smcs(env, iteration_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f env.noisycartpole_step [states, actions, rewards] = smcs(env, iteration_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %lprun -f multivariate_normal.logpdf [states, actions, rewards] = smcs(env, iteration_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Probability of Mean in a Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4682446775387277"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multivariate_normal.logpdf(1,mean=1,cov=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4682446775387277"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(1/2) * np.log(2*np.pi*(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covariance = np.array([[1,0],[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8378770664093453"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multivariate_normal.logpdf([1,1],mean=[1,1],cov=covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8378770664093453"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1/2) * np.log(np.linalg.det(2*np.pi*covariance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.44234151813963"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multivariate_normal.logpdf(states[0,0],mean=states[0,0],cov=env.transition_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.442341518139628"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1/2) * np.log(np.linalg.det(2*np.pi*env.transition_covariance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisycartpole_step(state, action):\n",
    "\n",
    "        next_state_mean, reward, done, info = env.cartpole_step(state, action)   # CartPole Step\n",
    "\n",
    "        noise = np.zeros(env.stateDimension)\n",
    "        if not done:\n",
    "            noise = np.random.randn(env.stateDimension) * env.transition_sigmas        # Adding Noise\n",
    "        next_state = next_state_mean + noise\n",
    "\n",
    "        logp = multivariate_normal.logpdf(next_state, mean=next_state_mean, cov=env.transition_covariance)\n",
    "\n",
    "        return next_state, reward, done, logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisycartpole_step_optimized(state, action):\n",
    "\n",
    "        next_state_mean, reward, done, info = env.cartpole_step(state, action)   # CartPole Step\n",
    "\n",
    "        if not done:\n",
    "            noise = np.random.randn(env.stateDimension) * env.transition_sigmas        # Adding Noise\n",
    "            logp = multivariate_normal.logpdf( next_state_mean + noise, mean=next_state_mean, cov=env.transition_covariance)\n",
    "        else:\n",
    "            noise = np.zeros(env.stateDimension)\n",
    "            logp = env.logp_mean\n",
    "        \n",
    "        return next_state_mean + noise, reward, done, logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 958 ms\n"
     ]
    }
   ],
   "source": [
    "%time _ = [noisycartpole_step(states[0,-1], actions[0,-1]) for x in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.1 ms\n"
     ]
    }
   ],
   "source": [
    "%time _ = [noisycartpole_step_optimized(states[0,-1], actions[0,-1]) for x in range(10000)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
