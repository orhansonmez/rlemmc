{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Cart Pole\n",
    "\n",
    "This is the noisy version of the CartPole-v0 environment of OpenAI.  \n",
    "https://gym.openai.com/envs/CartPole-v0  \n",
    "https://github.com/openai/gym/wiki/CartPole-v0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoisyCartPoleEnvironment:\n",
    "    \n",
    "    stateDimension = 4\n",
    "    actionDimension = 1\n",
    "    actionSpace = range(2)\n",
    "    transitionSigmas = [ 1e-2, 1e-5, 1e-2, 1e-5 ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def cartpole_reset(self):\n",
    "        state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        return np.array(state)\n",
    "    \n",
    "    # Extracted from OpenAI environment CartPole-v0\n",
    "    def cartpole_step(self, state, action):\n",
    "\n",
    "        gravity = 9.8\n",
    "        masscart = 1.0\n",
    "        masspole = 0.1\n",
    "        total_mass = (masspole + masscart)\n",
    "        length = 0.5 # actually half the pole's length\n",
    "        polemass_length = (masspole * length)\n",
    "        force_mag = 10.0\n",
    "        tau = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        x_threshold = 2.4\n",
    "\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "\n",
    "        already_done =  x < -x_threshold \\\n",
    "            or x > x_threshold \\\n",
    "            or theta < -theta_threshold_radians \\\n",
    "            or theta > theta_threshold_radians\n",
    "        already_done = bool(already_done)\n",
    "\n",
    "        if already_done:\n",
    "\n",
    "            next_state = state\n",
    "            reward = 0\n",
    "            done = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            force = force_mag if action==1 else -force_mag\n",
    "            costheta = math.cos(theta)\n",
    "            sintheta = math.sin(theta)\n",
    "            temp = (force + polemass_length * theta_dot * theta_dot * sintheta) / total_mass\n",
    "            thetaacc = (gravity * sintheta - costheta* temp) / (length * (4.0/3.0 - masspole * costheta * costheta / total_mass))\n",
    "            xacc  = temp - polemass_length * thetaacc * costheta / total_mass\n",
    "            x  = x + tau * x_dot\n",
    "            x_dot = x_dot + tau * xacc\n",
    "            theta = theta + tau * theta_dot\n",
    "            theta_dot = theta_dot + tau * thetaacc\n",
    "            next_state = np.array([x,x_dot,theta,theta_dot])\n",
    "\n",
    "            reward = 1\n",
    "\n",
    "            done =  x < -x_threshold \\\n",
    "                or x > x_threshold \\\n",
    "                or theta < -theta_threshold_radians \\\n",
    "                or theta > theta_threshold_radians\n",
    "            done = bool(done)\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def noisycartpole_reset(self):\n",
    "        return self.cartpole_reset()\n",
    "\n",
    "    def noisycartpole_step(self, state, action):\n",
    "\n",
    "        next_state_mean, reward, done, info = self.cartpole_step(state, action)   # CartPole Step\n",
    "\n",
    "        noise = np.zeros(self.stateDimension)\n",
    "        if not done:\n",
    "            noise = np.random.randn(self.stateDimension) * self.transitionSigmas        # Adding Noise\n",
    "        next_state = next_state_mean + noise\n",
    "\n",
    "        logp = multivariate_normal.logpdf(next_state, mean=next_state_mean, cov=np.diagflat(self.transitionSigmas))\n",
    "\n",
    "        return next_state, reward, done, logp\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.noisycartpole_reset()\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        return self.noisycartpole_step(state, action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = NoisyCartPoleEnvironment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trajectory2tuples(states, actions):\n",
    "\n",
    "    # Dimensions\n",
    "    [sample_count, horizon, state_dimension] = states.shape\n",
    "    [_, _, action_dimension] = actions.shape\n",
    "\n",
    "    # Reshape Inputs and Targets\n",
    "    inputs = np.reshape(states, (sample_count*horizon, state_dimension))\n",
    "    targets = np.reshape(actions, (sample_count*horizon, action_dimension))\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(states, color='red', n=0):\n",
    "\n",
    "    [sample_count, _, _] = states.shape\n",
    "\n",
    "    if n==0:\n",
    "        samples_drawn = range(sample_count)\n",
    "    else:\n",
    "        samples_drawn = np.random.choice(sample_count, n)\n",
    "        \n",
    "    for s in samples_drawn:\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], '-', color=color)\n",
    "        plt.plot(states[s, :, 0], states[s, :, 2], 'o', color=color, markersize=2)\n",
    "        plt.plot(states[s, -1, 0], states[s, -1, 2], 'o', color=color, markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_iteration(states, selected=None, n=0):\n",
    "    plot_trajectories(states, color='red', n=n)\n",
    "    if selected is not None:\n",
    "        plot_trajectories(selected, color='green', n=n)\n",
    "    \n",
    "    plt.vlines(0, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    # plt.vlines(-2.4, -0.25, 0.25, linestyle='dotted')\n",
    "    \n",
    "    plt.hlines(0, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    plt.hlines(-0.21, -2.4, 2.4, linestyle='dotted')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rollout_trajectories(env, policy, horizon, sample_count=1, init=None):\n",
    "\n",
    "    # States and Actions\n",
    "    states = np.zeros((sample_count, horizon, env.stateDimension))\n",
    "    actions = np.zeros((sample_count, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((sample_count, horizon))\n",
    "    logp = np.zeros(sample_count)\n",
    "    \n",
    "    # Sample Trajectories\n",
    "    for t in range(horizon):\n",
    "        \n",
    "        logp_step_transition = np.zeros((sample_count))\n",
    "\n",
    "        # Initialization\n",
    "        if t == 0:\n",
    "            if init is None:\n",
    "                states[:,t,:] = [ env.reset() for i in range(sample_count) ]\n",
    "            else:\n",
    "                states[:,t,:] = init\n",
    "                \n",
    "        # Transition and Reward\n",
    "        else:\n",
    "            for s in range(sample_count):\n",
    "                states[s, t, :], rewards[s,t-1], _1, logp_step_transition[s] = env.step(states[s, t-1, :], actions[s, t-1, :])\n",
    "        \n",
    "        # Action Selection\n",
    "        actions_unshaped, logp_step_policy = policy.query(states[:, t, :])\n",
    "        actions[:,t,:] = actions_unshaped.reshape(sample_count, env.actionDimension)\n",
    "        \n",
    "        # Log Probability of Sampling\n",
    "        logp += logp_step_transition + logp_step_policy\n",
    "        \n",
    "    for s in range(sample_count):\n",
    "        _, rewards[s, horizon-1], _1, _2 = env.step(states[s, horizon-1, :], actions[s, horizon-1, :])\n",
    "    \n",
    "    return states, actions, rewards, logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SciKitPolicy():\n",
    "\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def query(self, states):\n",
    "        if len(states.shape) == 1:\n",
    "            states = states.reshape(1, -1)\n",
    "        return self.method.predict(states), np.zeros(states.shape[0])\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        self.method.fit(inputs, targets)\n",
    "\n",
    "    def m_step(self, states, actions):\n",
    "\n",
    "        # States/Actions -> Inputs/Targets\n",
    "        inputs, targets = trajectory2tuples(states, actions)\n",
    "\n",
    "        # Train kNN\n",
    "        self.train(inputs, targets.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KnnPolicyDiscrete(SciKitPolicy):\n",
    "    def __init__(self, k, weights='distance'):\n",
    "        self.method = KNeighborsClassifier(n_neighbors=k, weights=weights, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UniformPolicyDiscrete():\n",
    "\n",
    "    def __init__(self, choices):\n",
    "        self.choices = choices\n",
    "\n",
    "    def query(self, states):\n",
    "        return np.random.choice(self.choices, size=states.shape[0]), np.zeros(states.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Sampling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temperatures = [0, 0.5, 1, 5, 10]\n",
    "temperature_count = len(temperatures)\n",
    "\n",
    "def eta(n):\n",
    "    return temperatures[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proposal_prior(states_previous, actions_previous, rewards_previous, logp_previous, policy):\n",
    "    return rollout_trajectories(env, policy, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simcmc(env, policy):\n",
    "    \n",
    "    states = np.zeros((temperature_count, sample_count, horizon, env.stateDimension))\n",
    "    actions = np.zeros((temperature_count, sample_count, horizon, env.actionDimension))\n",
    "    rewards = np.zeros((temperature_count, sample_count, horizon))\n",
    "    logp = np.zeros((temperature_count, sample_count))\n",
    "    weights_log = np.zeros((temperature_count, sample_count))\n",
    "\n",
    "    # Initial Trajectories\n",
    "    states[0], actions[0], rewards[0], logp[0] = \\\n",
    "        rollout_trajectories(env, policy, horizon, sample_count)\n",
    "\n",
    "    for s in range(sample_count):\n",
    "        for n in range(1,temperature_count):\n",
    "\n",
    "            # Sample Ancestor\n",
    "            a = np.random.randint(s+1)\n",
    "            # print(\"Sample {0!s} Brigde {1!s} : Ancestor <- {2!s}\".format(s,n,a))\n",
    "\n",
    "            # Sample Candidate \n",
    "            states_candidate, actions_candidate, rewards_candidate, logp_candidate = \\\n",
    "                proposal_prior(states[n-1,a], actions[n-1,a], rewards[n-1,a], logp[n-1,a], policy)\n",
    "\n",
    "            # Calculate Weight of the Candidate\n",
    "            reward_candidate = np.sum(rewards_candidate) / horizon\n",
    "            reward_ancestor = np.sum(rewards[n-1,a,:]) / horizon\n",
    "            logp_ancestor = logp[n-1,a]\n",
    "            weights_log_candidate = np.log(reward_candidate)*eta(n) - np.log(reward_ancestor)*eta(n-1) - logp_ancestor\n",
    "\n",
    "            # Calculate Acceptance Rate\n",
    "            if s > 0:\n",
    "                acceptance = np.exp(weights_log_candidate - weights_log[n,s-1])\n",
    "            else:\n",
    "                acceptance = 1\n",
    "\n",
    "            # Accept or Reject\n",
    "            if np.random.rand() < acceptance:\n",
    "                states[n,s] = states_candidate\n",
    "                actions[n,s] = actions_candidate\n",
    "                rewards[n,s] = rewards_candidate\n",
    "                logp[n,s] = logp_candidate\n",
    "                weights_log[n,s] = weights_log_candidate\n",
    "            else:\n",
    "                states[n,s] = states[n,s-1]\n",
    "                actions[n,s] = actions[n,s-1]\n",
    "                rewards[n,s] = rewards[n,s-1]\n",
    "                logp[n,s] = logp[n,s-1]\n",
    "                weights_log[n,s] = weights_log[n,s-1]\n",
    "         \n",
    "         \n",
    "    return states[-1], actions[-1], rewards[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment  \n",
    "horizon = 250\n",
    "\n",
    "# Inference\n",
    "iteration_count = 50\n",
    "sample_count = 100\n",
    "\n",
    "# Policy\n",
    "kNearest = 5\n",
    "policy_sample_count = sample_count\n",
    "selectedPolicy = KnnPolicyDiscrete(kNearest)\n",
    "\n",
    "# Plot\n",
    "rendering_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLEMMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Average Reward : 53.580000000000005\n",
      "Iteration 2 - Average Reward : 147.35000000000002\n",
      "Iteration 3 - Average Reward : 137.18\n",
      "Iteration 4 - Average Reward : 171.15\n",
      "Iteration 5 - Average Reward : 233.13\n",
      "Iteration 6 - Average Reward : 236.67\n",
      "Iteration 7 - Average Reward : 242.2\n",
      "Iteration 8 - Average Reward : 244.09\n",
      "Iteration 9 - Average Reward : 244.71\n",
      "Iteration 10 - Average Reward : 231.43\n",
      "Iteration 11 - Average Reward : 245.85000000000002\n",
      "Iteration 12 - Average Reward : 239.03\n",
      "Iteration 13 - Average Reward : 244.93\n",
      "Iteration 14 - Average Reward : 244.23\n",
      "Iteration 15 - Average Reward : 242.64\n",
      "Iteration 16 - Average Reward : 239.74\n",
      "Iteration 17 - Average Reward : 238.38\n",
      "Iteration 18 - Average Reward : 234.3\n",
      "Iteration 19 - Average Reward : 244.60000000000002\n",
      "Iteration 20 - Average Reward : 247.88\n",
      "Iteration 21 - Average Reward : 242.54999999999998\n",
      "Iteration 22 - Average Reward : 238.26\n",
      "Iteration 23 - Average Reward : 248.22\n",
      "Iteration 24 - Average Reward : 247.93\n",
      "Iteration 25 - Average Reward : 247.98999999999998\n",
      "Iteration 26 - Average Reward : 242.47\n",
      "Iteration 27 - Average Reward : 240.13000000000002\n",
      "Iteration 28 - Average Reward : 243.82000000000002\n",
      "Iteration 29 - Average Reward : 240.54\n",
      "Iteration 30 - Average Reward : 240.17\n",
      "Iteration 31 - Average Reward : 234.73\n",
      "Iteration 32 - Average Reward : 235.71\n",
      "Iteration 33 - Average Reward : 238.74\n",
      "Iteration 34 - Average Reward : 241.98\n",
      "Iteration 35 - Average Reward : 247.54000000000002\n",
      "Iteration 36 - Average Reward : 237.03\n",
      "Iteration 37 - Average Reward : 236.29\n",
      "Iteration 38 - Average Reward : 241.62\n",
      "Iteration 39 - Average Reward : 237.55\n",
      "Iteration 40 - Average Reward : 245.31\n",
      "Iteration 41 - Average Reward : 245.09\n",
      "Iteration 42 - Average Reward : 242.28\n",
      "Iteration 43 - Average Reward : 237.53\n",
      "Iteration 44 - Average Reward : 235.94\n",
      "Iteration 45 - Average Reward : 239.93\n",
      "Iteration 46 - Average Reward : 246.03\n",
      "Iteration 47 - Average Reward : 242.91\n",
      "Iteration 48 - Average Reward : 242.73\n",
      "Iteration 49 - Average Reward : 245.98999999999998\n",
      "Iteration 50 - Average Reward : 239.14\n"
     ]
    }
   ],
   "source": [
    "iteration_rewards = []\n",
    "\n",
    "for i in range(iteration_count):\n",
    "    \n",
    "    if i == 0:\n",
    "        iteration_policy = UniformPolicyDiscrete(env.actionSpace)\n",
    "    else:\n",
    "        iteration_policy = selectedPolicy\n",
    "    \n",
    "    # E-Step\n",
    "    [states, actions, rewards] = simcmc(env, iteration_policy)\n",
    "\n",
    "    # M-Step\n",
    "    selectedPolicy.m_step(states, actions)\n",
    "        \n",
    "    # Average Reward\n",
    "    iteration_rewards.append(np.mean(rewards) * horizon)\n",
    "    print( f'Iteration {i+1} - Average Reward : {iteration_rewards[i]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkYBT0IuIBNz/A9pQRamRuxJlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20715f0fc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iteration_rewards)\n",
    "plt.hlines(np.min([horizon,195]), 0, iteration_count, linestyle='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "def render_state(env, t):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Timestep : %s\" %(env.spec.id, t))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if rendering_enabled:\n",
    "    env = gym.make('CartPole-v0')\n",
    "\n",
    "    state = env.reset()\n",
    "    for t in range(horizon):\n",
    "        render_state(env,t)\n",
    "        state,_,done,_ = env.step(int(iteration_policy.query(state)))\n",
    "        if done:\n",
    "            break        \n",
    "    render_state(env,t)\n",
    "\n",
    "    env.render(close=True)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
